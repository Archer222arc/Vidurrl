{
  "state_dim": 210,
  "action_dim": 4,
  "comment": "UPDATED: Increased from 198 to 210 to match enhanced StateBuilder with queue delay features",
  "learning_rate": 1e-4,
  "weight_decay": 1e-4,
  "batch_size": 512,
  "epochs": 100,
  "validation_split": 0.2,
  "early_stopping_patience": 20,
  "save_interval": 10,
  "device": "cpu",
  "output_dir": "./outputs/standalone_pretrain",
  "hidden_size": 320,
  "layer_N": 3,
  "gru_layers": 3,
  "enable_decoupled": false,
  "feature_projection_dim": 640,
  "network_architecture": {
    "type": "enhanced_gru",
    "hidden_size": 320,
    "num_gru_layers": 3,
    "dropout_rate": 0.1,
    "use_layer_norm": true,
    "use_residual": true,
    "activation": "gelu"
  },
  "advanced_features": {
    "strategy_embedding": {
      "enable": true,
      "embedding_dim": 128,
      "num_strategies": 3
    },
    "attention_mechanism": {
      "enable": true,
      "num_heads": 4,
      "attention_dropout": 0.1
    },
    "feature_fusion": {
      "enable": true,
      "fusion_type": "attention_weighted"
    }
  },
  "cross_replica_attention": {
    "enable": true,
    "feature_dim": 320,
    "num_heads": 4,
    "dropout": 0.1,
    "use_layer_norm": true,
    "dimension_handling": {
      "enable_adaptive_projection": true,
      "minimum_feature_dim": 64,
      "ensure_head_divisibility": true
    },
    "comment": "Cross-replica attention to match PPO architecture"
  },
  "temporal_lstm": {
    "enable": true,
    "feature_chunks": 4,
    "bidirectional": true,
    "hidden_size_ratio": 0.25,
    "residual_connections": true,
    "comment": "Temporal LSTM for sequence modeling to match PPO architecture"
  },
  "regularization": {
    "gradient_clip_norm": 1.0,
    "label_smoothing": 0.1,
    "mixup_alpha": 0.2
  },
  "optimization": {
    "optimizer": "adamw",
    "lr_scheduler": "cosine_annealing_with_restarts",
    "warmup_steps": 1000,
    "min_lr_ratio": 0.01,
    "t_0": 10,
    "t_mult": 2
  },
  "demo_collection": {
    "strategies": ["round_robin", "lor", "random"],
    "steps_per_strategy": 2000,
    "num_replicas": 4,
    "qps": 3.0,
    "include_variants": true
  },
  "data_augmentation": {
    "enabled": true,
    "noise_std": 0.01,
    "cutmix_probability": 0.1,
    "temporal_jitter": 0.05
  },
  "monitoring": {
    "track_attention_weights": true,
    "track_gradient_norms": true,
    "track_feature_distributions": true,
    "export_embeddings": true
  },
  "state_builder_enhanced": {
    "enable_queue_delay_features": true,
    "max_queue_requests": 4,
    "queue_delay_normalization": {
      "max_wait_time_seconds": 10.0,
      "urgency_scale": 10.0,
      "priority_weight": 1.0
    },
    "feature_names": [
      "oldest_request_wait_time",
      "avg_queue_wait_time",
      "queue_urgency_score"
    ],
    "comment": "Enhanced state features for queue delay awareness (same as PPO training)"
  },
  "statistics_stabilization": {
    "enable_statistics_stabilization": true,
    "stabilization_steps": 100,
    "stabilization_policy": "random",
    "collect_baseline_stats": true,
    "freeze_normalizers_during_stabilization": false,
    "enable_stabilization_logging": true,
    "stabilization_action_distribution": "uniform",
    "comment": "Separate from training warmup - this stabilizes state/reward normalizers before PPO starts"
  }
}