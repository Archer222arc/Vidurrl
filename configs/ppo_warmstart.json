{
  "training": {
    "num_replicas": 4,
    "qps": 2.5,
    "ppo_requests": 20000,
    "bc_epochs": 30,
    "demo_steps_per_policy": 700,
    "demo_policies": "round_robin lor random"
  },
  "model_dimensions": {
    "state_dim": 210,
    "action_dim": 4,
    "comment": "VERIFIED: StateBuilder calculates 210 dimensions (4 replicas * 50 features + 10 global features, queue delay features included in base 14)"
  },
  "state_dimension_compatibility": {
    "pretrain_state_dim": 200,
    "ppo_state_dim": 210,
    "status": "ENHANCED - New queue delay features added",
    "comment": "PPO uses enhanced 210-dimensional state vectors with queue delay features included in base features",
    "state_formula": "num_replicas * (14+8+28) + 10 = 4 * 50 + 10 = 210 (queue delay in base 14)",
    "new_features": ["oldest_request_wait_time", "avg_queue_wait_time", "queue_urgency_score"],
    "direct_transfer": false,
    "requires_adaptation": true
  },
  "network_compatibility": {
    "pretrain_hidden_size": 320,
    "pretrain_gru_layers": 3,
    "pretrain_num_layers": 3,
    "inherit_strategy_embeddings": true,
    "inherit_attention_weights": true,
    "freeze_feature_extractor": false,
    "adaptive_layer_init": true,
    "enable_gradual_unfreezing": true,
    "unfreezing_schedule": [500, 1000, 2000],
    "compatibility_status": "MATCHED - Actor inherits pretrain architecture"
  },
  "actor_critic_architecture": {
    "shared_backbone": false,
    "enable_cross_replica_attention": true,
    "num_replicas": 4,
    "attention_heads": 4,
    "actor": {
      "hidden_size": 320,
      "num_layers": 3,
      "use_gru": true,
      "gru_layers": 3,
      "output_activation": "softmax",
      "temperature_scaling": true,
      "exploration_bonus_layer": true,
      "comment": "Enhanced with cross-replica attention mechanism (PDF recommendation)"
    },
    "critic": {
      "hidden_size": 384,
      "num_layers": 4,
      "use_gru": true,
      "gru_layers": 3,
      "value_head_layers": 2,
      "value_normalization": true,
      "lightweight_head": true,
      "comment": "Optimized critic with potential shared encoder"
    },
    "cross_replica_attention": {
      "enable": true,
      "feature_dim": 320,
      "num_heads": 4,
      "dropout": 0.1,
      "use_layer_norm": true,
      "comment": "NEW: Cross-replica attention for explicit replica comparison"
    }
  },
  "ppo_config": {
    "lr": 0.0003,
    "gamma": 0.95,
    "gae_lambda": 0.95,
    "clip_ratio": 0.15,
    "epochs": 8,
    "rollout_len": 128,
    "minibatch_size": 64,
    "entropy_coef": 0.02,
    "entropy_schedule": {
      "enable": true,
      "initial": 0.02,
      "final": 0.0,
      "decay_steps": 40000,
      "decay_type": "linear"
    },
    "entropy_warmup_coef": 0.01,
    "stabilization_steps": 100,
    "max_grad_norm": 1.0
  },
  "reward_config": {
    "latency_weight": 1.5,
    "balance_penalty_weight": 0.2,
    "latency_threshold": 4.0,
    "latency_penalty_scale": 0.75,
    "load_balance_penalty": 1.2,
    "throughput_target": 2.0,
    "absolute_weight": 0.6,
    "delta_weight": 0.4,
    "alpha": 0.4,
    "beta": 0.4,
    "gamma": 0.3,
    "kappa": 0.25,
    "sigma": 1.2,
    "ema_alpha": 0.15,
    "reward_scaling": {
      "type": "adaptive",
      "scale_factor": 1.0,
      "clip_range": [-8.0, 8.0],
      "enable_soft_clipping": true,
      "soft_clip_threshold": 6.0,
      "comment": "IMPROVED: Expanded range and soft clipping to prevent reward saturation while maintaining gradients"
    },
    "latency_bonus": {
      "enable": true,
      "threshold_fraction": 0.8,
      "bonus_scale": 0.5,
      "comment": "NEW: Bonus reward for achieving latency significantly below SLO"
    }
  },
  "kl_regularization": {
    "target_kl": 0.1,
    "adaptive_kl": {
      "enable": true,
      "min_target": 0.05,
      "max_target": 0.2,
      "adaptation_rate": 0.02
    },
    "entropy_min": 0.2,
    "kl_coef": 0.2,
    "kl_ref_coef_initial": 0.6,
    "kl_ref_coef_final": 0.1,
    "kl_ref_decay_steps": 3000,
    "warmup_steps": 1500,
    "early_stopping": {
      "enable": true,
      "kl_threshold": 0.15,
      "patience_steps": 3
    },
    "comment": "IMPROVED: Adaptive KL targeting and early stopping for stable policy updates"
  },
  "statistics_stabilization": {
    "enable_statistics_stabilization": true,
    "stabilization_steps": 200,
    "stabilization_policy": "random",
    "collect_baseline_stats": true,
    "freeze_normalizers_during_stabilization": false,
    "enable_stabilization_logging": true,
    "stabilization_action_distribution": "uniform",
    "comment": "Separate from training warmup - this stabilizes state/reward normalizers before PPO starts"
  },
  "temperature_control": {
    "enable_dynamic_temperature": true,
    "base_temperature": 1.5,
    "min_temperature": 0.8,
    "max_temperature": 3.0,
    "qps_sensitivity": 0.05,
    "latency_sensitivity": 0.1,
    "adaptive_schedule": true
  },
  "enhanced_features": {
    "enable_enhanced_features": true,
    "state_history_window": 5,
    "qps_window": 10,
    "temporal_attention": {
      "enable": true,
      "num_heads": 4,
      "history_encoding": "lstm"
    },
    "feature_projection": {
      "enable": true,
      "projection_dim": 256,
      "use_layer_norm": true
    }
  },
  "state_builder_enhanced": {
    "enable_queue_delay_features": true,
    "max_queue_requests": 4,
    "queue_delay_normalization": {
      "max_wait_time_seconds": 10.0,
      "urgency_scale": 10.0,
      "priority_weight": 1.0
    },
    "feature_names": [
      "oldest_request_wait_time",
      "avg_queue_wait_time",
      "queue_urgency_score"
    ],
    "comment": "NEW: Enhanced state features for queue delay awareness (PDF recommendation)"
  },
  "curriculum_learning": {
    "enable": true,
    "stages": [
      {
        "name": "easy",
        "duration_requests": 10000,
        "qps_scale": 0.7,
        "latency_threshold_scale": 1.2,
        "reward_penalty_scale": 0.5
      },
      {
        "name": "medium",
        "duration_requests": 15000,
        "qps_scale": 1.0,
        "latency_threshold_scale": 1.0,
        "reward_penalty_scale": 0.75
      },
      {
        "name": "hard",
        "duration_requests": 15000,
        "qps_scale": 1.3,
        "latency_threshold_scale": 0.9,
        "reward_penalty_scale": 1.0
      }
    ],
    "comment": "NEW: Progressive difficulty to avoid getting stuck in local optima"
  },
  "advanced_optimization": {
    "learning_rate_schedule": {
      "type": "cosine_annealing_with_warmup",
      "warmup_steps": 1000,
      "min_lr_ratio": 0.1,
      "restart_period": 5000
    },
    "adaptive_batch_size": {
      "enable": true,
      "min_batch_size": 32,
      "max_batch_size": 128,
      "adaptation_rate": 0.1
    },
    "gradient_optimization": {
      "use_gradient_checkpointing": true,
      "accumulation_steps": 2,
      "clip_by_global_norm": true
    }
  },
  "checkpointing": {
    "enable_checkpoints": true,
    "checkpoint_dir": "./outputs/checkpoints",
    "checkpoint_interval": 128,
    "max_checkpoints": 5,
    "save_optimizer_state": true,
    "incremental_checkpoints": true
  },
  "parallel_training": {
    "enable": true,
    "num_envs": 4,
    "env_type": "subprocess",
    "shared_memory": true,
    "comment": "NEW: Parallel environments for better sample diversity and faster training"
  },
  "monitoring": {
    "tensorboard_port": 6006,
    "metrics_export_format": "csv",
    "metrics_export_interval": 50,
    "metrics_subsamples": 200000,
    "tail_latency_tracking": {
      "enable": true,
      "percentiles": [90, 95, 99],
      "window_size": 1000,
      "alert_threshold_p99": 5.0,
      "comment": "NEW: Explicit tail latency monitoring as recommended in PDF"
    },
    "safety_monitoring": {
      "enable": true,
      "max_latency_cutoff": 10.0,
      "episode_failure_penalty": -10.0,
      "queue_length_limit": 50,
      "comment": "NEW: Safety constraints to prevent catastrophic scheduling"
    },
    "advanced_metrics": {
      "track_policy_entropy": true,
      "track_value_function_variance": true,
      "track_action_distribution": true,
      "track_kl_divergence": true,
      "track_advantage_statistics": true,
      "track_reward_components": true,
      "track_tail_latencies": true,
      "track_queue_delay_features": true,
      "track_attention_weights": true,
      "track_replica_utilization_balance": true
    },
    "visualization": {
      "plot_attention_heatmaps": true,
      "plot_policy_evolution": true,
      "plot_reward_decomposition": true,
      "plot_tail_latency_trends": true
    }
  }
}