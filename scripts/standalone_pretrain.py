#!/usr/bin/env python3
"""
ç‹¬ç«‹é¢„è®­ç»ƒè„šæœ¬ - å¢å¼ºç‰ˆè¡Œä¸ºå…‹éš†

æ”¯æŒé•¿æ­¥éª¤ã€å¤šç­–ç•¥ã€å¤šé˜¶æ®µçš„é¢„è®­ç»ƒï¼Œç”Ÿæˆé«˜è´¨é‡çš„é¢„è®­ç»ƒæ¨¡å‹ä¾›PPO warmstartä½¿ç”¨ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
- æ”¯æŒå¤šè½®æ•°æ®æ”¶é›†å’Œå¢é‡è®­ç»ƒ
- å¯é…ç½®çš„è®­ç»ƒç­–ç•¥ç»„åˆ
- æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤
- è®­ç»ƒè¿›åº¦ç›‘æ§å’Œæ—©åœ
- æ ‡å‡†åŒ–çš„æ¨¡å‹è¾“å‡ºæ ¼å¼
"""

import argparse
import json
import pickle
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter


def _bootstrap_repo_path() -> None:
    """Ensure imports resolve to the current repo checkout."""
    repo_root = Path(__file__).resolve().parent.parent
    conflict_root = repo_root.parent / "Vidur"

    def _same_path(a: str, b: Path) -> bool:
        try:
            return Path(a).resolve() == b.resolve()
        except (OSError, RuntimeError):
            return False

    sys.path[:] = [p for p in sys.path if not _same_path(p, conflict_root)]

    repo_root_str = str(repo_root)
    if repo_root_str not in sys.path:
        sys.path.insert(0, repo_root_str)


_bootstrap_repo_path()

from src.core.models.actor_critic import ActorCritic
from src.core.utils.normalizers import RunningNormalizer


class EnhancedDemoDataset(Dataset):
    """å¢å¼ºçš„ç¤ºæ•™æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§æ•°æ®å¢å¼ºç­–ç•¥"""

    def __init__(
        self,
        demo_data: List[Dict[str, Any]],
        normalizer: Optional[RunningNormalizer] = None,
        augment_data: bool = True,
        noise_std: float = 0.01
    ):
        self.demo_data = demo_data
        self.normalizer = normalizer
        self.augment_data = augment_data
        self.noise_std = noise_std

        # é¢„å¤„ç†æ•°æ®
        self.states = []
        self.actions = []
        self.weights = []  # æ ·æœ¬æƒé‡ï¼Œç”¨äºå¹³è¡¡ä¸åŒç­–ç•¥çš„æ•°æ®

        self._preprocess_data()

    def _preprocess_data(self):
        """é¢„å¤„ç†å’Œç»Ÿè®¡æ•°æ®"""
        strategy_counts = {}

        for item in self.demo_data:
            state = np.array(item['state'], dtype=np.float32)
            action = item['action']
            strategy = item.get('strategy', 'unknown')

            # å½’ä¸€åŒ–çŠ¶æ€
            if self.normalizer:
                state = self.normalizer.normalize(state)

            self.states.append(state)
            self.actions.append(action)

            # ç»Ÿè®¡ç­–ç•¥åˆ†å¸ƒ
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        # è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆå¹³è¡¡ä¸åŒç­–ç•¥ï¼‰
        total_samples = len(self.demo_data)
        num_strategies = len(strategy_counts)

        for item in self.demo_data:
            strategy = item.get('strategy', 'unknown')
            strategy_weight = total_samples / (num_strategies * strategy_counts[strategy])
            self.weights.append(strategy_weight)

        print(f"ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆ:")
        print(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   - ç­–ç•¥åˆ†å¸ƒ: {strategy_counts}")
        print(f"   - æ•°æ®å¢å¼º: {'å¯ç”¨' if self.augment_data else 'ç¦ç”¨'}")

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        state = self.states[idx].copy()
        action = self.actions[idx]
        weight = self.weights[idx]

        # æ•°æ®å¢å¼ºï¼šæ·»åŠ å°‘é‡å™ªå£°
        if self.augment_data and self.noise_std > 0:
            noise = np.random.normal(0, self.noise_std, state.shape)
            state = state + noise

        return torch.tensor(state, dtype=torch.float32), action, weight


class StandalonePretrainer:
    """ç‹¬ç«‹é¢„è®­ç»ƒå™¨ - æ”¯æŒé•¿æœŸè®­ç»ƒå’Œæ¨¡å‹ç®¡ç†"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_size: int = 256,
        layer_N: int = 3,
        gru_layers: int = 2,
        learning_rate: float = 1e-4,
        device: str = "cpu",
        output_dir: str = "./outputs/standalone_pretrain"
    ):
        self.device = torch.device(device)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ¨¡å‹é…ç½®
        self.model_config = {
            'state_dim': state_dim,
            'action_dim': action_dim,
            'hidden_size': hidden_size,
            'layer_N': layer_N,
            'gru_layers': gru_layers,
        }

        # åˆ›å»ºActor-Criticç½‘ç»œ
        self.actor_critic = ActorCritic(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_size=hidden_size,
            layer_N=layer_N,
            gru_layers=gru_layers,
        ).to(self.device)

        # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆåªè®­ç»ƒActoréƒ¨åˆ†ï¼‰- ç›´æ¥è®¿é—®ï¼Œç¼ºå¤±å±æ€§ä¼šè‡ªç„¶æŠ¥é”™
        actor_params = []
        if self.actor_critic.enable_decoupled:
            # è§£è€¦æ¶æ„ï¼šActorå‚æ•°åœ¨actor_branchå’Œactor_headä¸­
            actor_params.extend(self.actor_critic.actor_branch.parameters())
            actor_params.extend(self.actor_critic.actor_head.parameters())
        else:
            # è€¦åˆæ¶æ„ï¼šActorå‚æ•°åœ¨actorä¸­
            actor_params.extend(self.actor_critic.actor.parameters())

        self.optimizer = torch.optim.AdamW(actor_params, lr=learning_rate, weight_decay=1e-5)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.7, patience=10, verbose=True
        )

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0

        # TensorBoardè®°å½•
        self.writer = SummaryWriter(self.output_dir / "tensorboard")

        print(f"ğŸ¤– ç‹¬ç«‹é¢„è®­ç»ƒå™¨å·²åˆ›å»º:")
        print(f"   - æ¨¡å‹é…ç½®: {self.model_config}")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")

    def collect_extended_demo_data(
        self,
        strategies: List[str],
        steps_per_strategy: int,
        num_replicas: int = 4,
        qps: float = 3.0,
        include_variants: bool = True
    ) -> str:
        """æ”¶é›†æ‰©å±•çš„ç¤ºæ•™æ•°æ®"""
        print(f"ğŸ“Š å¼€å§‹æ”¶é›†æ‰©å±•ç¤ºæ•™æ•°æ®...")
        print(f"   - ç­–ç•¥: {strategies}")
        print(f"   - æ¯ç­–ç•¥æ­¥æ•°: {steps_per_strategy}")
        print(f"   - å‰¯æœ¬æ•°: {num_replicas}")
        print(f"   - QPS: {qps}")

        demo_file = self.output_dir / f"extended_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"

        # TODO: è¿™é‡Œåº”è¯¥è°ƒç”¨æ•°æ®æ”¶é›†æ¨¡å—
        # æš‚æ—¶æ¨¡æ‹Ÿæ•°æ®æ”¶é›†è¿‡ç¨‹
        demo_data = []
        for strategy in strategies:
            for step in range(steps_per_strategy):
                # æ¨¡æ‹Ÿç¤ºæ•™æ•°æ®é¡¹
                demo_item = {
                    'state': np.random.randn(64).tolist(),  # ç¤ºä¾‹çŠ¶æ€ç»´åº¦
                    'action': np.random.randint(0, 4),       # ç¤ºä¾‹åŠ¨ä½œ
                    'strategy': strategy,
                    'step': step,
                    'timestamp': time.time()
                }
                demo_data.append(demo_item)

        # ä¿å­˜æ•°æ®
        data_package = {
            'demo_data': demo_data,
            'stats': {
                'total_samples': len(demo_data),
                'strategies': strategies,
                'steps_per_strategy': steps_per_strategy,
                'state_dim': 64,  # ç¤ºä¾‹
                'action_dim': 4,  # ç¤ºä¾‹
            },
            'metadata': {
                'collection_time': datetime.now().isoformat(),
                'num_replicas': num_replicas,
                'qps': qps,
                'include_variants': include_variants,
            }
        }

        with open(demo_file, 'wb') as f:
            pickle.dump(data_package, f)

        print(f"âœ… ç¤ºæ•™æ•°æ®æ”¶é›†å®Œæˆ: {demo_file}")
        return str(demo_file)

    def train_epoch(self, dataloader: DataLoader) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.actor_critic.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (states, actions, weights) in enumerate(dataloader):
            states = states.to(self.device)
            actions = torch.tensor(actions, dtype=torch.long).to(self.device)
            weights = torch.tensor(weights, dtype=torch.float32).to(self.device)

            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                # åˆå§‹åŒ–éšè—çŠ¶æ€
                batch_size = states.shape[0]
                hxs = torch.zeros(batch_size, self.actor_critic.gru_layers * self.actor_critic.hidden_size,
                                device=self.device)
                mask = torch.ones(batch_size, 1, device=self.device)

            # è·å–Actorè¾“å‡º
            action_logits, _, _, _ = self.actor_critic.act_value(states, hxs, mask)

            # è®¡ç®—åŠ æƒäº¤å‰ç†µæŸå¤±
            loss = F.cross_entropy(action_logits, actions, reduction='none')
            weighted_loss = (loss * weights).mean()

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            weighted_loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=1.0)

            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += weighted_loss.item()
            pred = action_logits.argmax(dim=1)
            correct += (pred == actions).sum().item()
            total += actions.size(0)

            # è®°å½•æ‰¹æ¬¡æŒ‡æ ‡
            if batch_idx % 100 == 0:
                self.writer.add_scalar('Train/Batch_Loss', weighted_loss.item(),
                                     self.epoch * len(dataloader) + batch_idx)

        avg_loss = total_loss / len(dataloader)
        accuracy = correct / total if total > 0 else 0.0

        return avg_loss, accuracy

    def validate(self, dataloader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹"""
        self.actor_critic.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for states, actions, weights in dataloader:
                states = states.to(self.device)
                actions = torch.tensor(actions, dtype=torch.long).to(self.device)
                weights = torch.tensor(weights, dtype=torch.float32).to(self.device)

                # åˆå§‹åŒ–éšè—çŠ¶æ€
                batch_size = states.shape[0]
                hxs = torch.zeros(batch_size, self.actor_critic.gru_layers * self.actor_critic.hidden_size,
                                device=self.device)
                mask = torch.ones(batch_size, 1, device=self.device)

                # è·å–Actorè¾“å‡º
                action_logits, _, _, _ = self.actor_critic.act_value(states, hxs, mask)

                # è®¡ç®—æŸå¤±
                loss = F.cross_entropy(action_logits, actions, reduction='none')
                weighted_loss = (loss * weights).mean()

                total_loss += weighted_loss.item()
                pred = action_logits.argmax(dim=1)
                correct += (pred == actions).sum().item()
                total += actions.size(0)

        avg_loss = total_loss / len(dataloader)
        accuracy = correct / total if total > 0 else 0.0

        return avg_loss, accuracy

    def train(
        self,
        demo_files: List[str],
        epochs: int = 100,
        batch_size: int = 512,
        validation_split: float = 0.2,
        early_stopping_patience: int = 20,
        save_interval: int = 10
    ) -> Dict[str, List[float]]:
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹ç‹¬ç«‹é¢„è®­ç»ƒ...")
        print(f"   - è®­ç»ƒepochs: {epochs}")
        print(f"   - æ‰¹å¤§å°: {batch_size}")
        print(f"   - éªŒè¯é›†æ¯”ä¾‹: {validation_split}")
        print(f"   - æ—©åœpatience: {early_stopping_patience}")

        # åŠ è½½æ‰€æœ‰ç¤ºæ•™æ•°æ®
        all_demo_data = []
        for demo_file in demo_files:
            with open(demo_file, 'rb') as f:
                data = pickle.load(f)
                all_demo_data.extend(data['demo_data'])

        print(f"ğŸ“‚ åŠ è½½ç¤ºæ•™æ•°æ®å®Œæˆ: å…±{len(all_demo_data)}ä¸ªæ ·æœ¬")

        # æ•°æ®é›†åˆ†å‰²
        split_idx = int(len(all_demo_data) * (1 - validation_split))
        train_data = all_demo_data[:split_idx]
        val_data = all_demo_data[split_idx:]

        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = EnhancedDemoDataset(train_data, augment_data=True)
        val_dataset = EnhancedDemoDataset(val_data, augment_data=False)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rate': []
        }

        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"   - éªŒè¯é›†: {len(val_data)} æ ·æœ¬")

        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.epoch = epoch
            start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)

            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']

            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['learning_rate'].append(current_lr)

            # TensorBoardè®°å½•
            self.writer.add_scalars('Loss', {'Train': train_loss, 'Val': val_loss}, epoch)
            self.writer.add_scalars('Accuracy', {'Train': train_acc, 'Val': val_acc}, epoch)
            self.writer.add_scalar('Learning_Rate', current_lr, epoch)

            # æ—©åœæ£€æŸ¥
            if val_loss < self.best_loss:
                self.best_loss = val_loss
                self.patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_model("best_model.pt", include_training_state=True)
            else:
                self.patience_counter += 1

            # å®šæœŸä¿å­˜
            if (epoch + 1) % save_interval == 0:
                self.save_model(f"checkpoint_epoch_{epoch+1}.pt", include_training_state=True)

            # è¾“å‡ºè¿›åº¦
            epoch_time = time.time() - start_time
            print(f"Epoch {epoch+1:3d}/{epochs} | "
                  f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f} | "
                  f"LR: {current_lr:.2e} | Time: {epoch_time:.1f}s")

            # æ—©åœ
            if self.patience_counter >= early_stopping_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ (patience={early_stopping_patience})")
                break

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model("final_model.pt", include_training_state=False)

        # ä¿å­˜è®­ç»ƒå†å²
        history_file = self.output_dir / "training_history.json"
        with open(history_file, 'w') as f:
            json.dump(history, f, indent=2)

        self.writer.close()
        print(f"ğŸ‰ ç‹¬ç«‹é¢„è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")

        return history

    def save_model(self, filename: str, include_training_state: bool = False):
        """ä¿å­˜æ¨¡å‹"""
        save_path = self.output_dir / filename

        save_dict = {
            'model_state_dict': self.actor_critic.state_dict(),
            'model_config': self.model_config,
            'training_metadata': {
                'epoch': self.epoch,
                'best_loss': self.best_loss,
                'save_time': datetime.now().isoformat(),
                'model_type': 'standalone_pretrained',
                'architecture': 'actor_critic',
            }
        }

        if include_training_state:
            save_dict.update({
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'patience_counter': self.patience_counter,
            })

        torch.save(save_dict, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")

    def load_model(self, model_path: str, load_training_state: bool = False):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)

        # åŠ è½½æ¨¡å‹æƒé‡
        self.actor_critic.load_state_dict(checkpoint['model_state_dict'])

        # éªŒè¯æ¨¡å‹é…ç½®
        saved_config = checkpoint['model_config']
        if saved_config != self.model_config:
            print(f"âš ï¸ æ¨¡å‹é…ç½®ä¸åŒ¹é…:")
            print(f"   å½“å‰: {self.model_config}")
            print(f"   ä¿å­˜: {saved_config}")

        # åŠ è½½è®­ç»ƒçŠ¶æ€
        if load_training_state and 'optimizer_state_dict' in checkpoint:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.epoch = checkpoint.get('epoch', 0)
            self.best_loss = checkpoint.get('best_loss', float('inf'))
            self.patience_counter = checkpoint.get('patience_counter', 0)

        metadata = checkpoint.get('training_metadata', {})
        print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {model_path}")
        print(f"   - è®­ç»ƒepoch: {metadata.get('epoch', 'unknown')}")
        print(f"   - æœ€ä½³æŸå¤±: {metadata.get('best_loss', 'unknown')}")
        print(f"   - ä¿å­˜æ—¶é—´: {metadata.get('save_time', 'unknown')}")


def main():
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹é¢„è®­ç»ƒè„šæœ¬ - å¢å¼ºç‰ˆè¡Œä¸ºå…‹éš†")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)")
    parser.add_argument("--demo-files", nargs='+', help="ç¤ºæ•™æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--collect-demo", action='store_true', help="æ˜¯å¦æ”¶é›†æ–°çš„ç¤ºæ•™æ•°æ®")
    parser.add_argument("--strategies", nargs='+', default=["round_robin", "lor", "random", "shortest_queue"],
                       help="ç¤ºæ•™ç­–ç•¥åˆ—è¡¨")
    parser.add_argument("--steps-per-strategy", type=int, default=2000, help="æ¯ç­–ç•¥æ”¶é›†æ­¥æ•°")
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=512, help="æ‰¹å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--hidden-size", type=int, default=256, help="éšè—å±‚å¤§å°")
    parser.add_argument("--layer-N", type=int, default=3, help="MLPå±‚æ•°")
    parser.add_argument("--gru-layers", type=int, default=2, help="GRUå±‚æ•°")
    parser.add_argument("--output-dir", type=str, default="./outputs/standalone_pretrain", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--resume", type=str, help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")

    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    if args.config and Path(args.config).exists():
        with open(args.config, 'r') as f:
            config = json.load(f)
        # é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        for key, value in config.items():
            if hasattr(args, key):
                setattr(args, key, value)

    # åˆ›å»ºé¢„è®­ç»ƒå™¨
    pretrainer = StandalonePretrainer(
        state_dim=64,  # è¿™é‡Œåº”è¯¥ä»å®é™…é…ç½®è·å–
        action_dim=4,  # è¿™é‡Œåº”è¯¥ä»å®é™…é…ç½®è·å–
        hidden_size=args.hidden_size,
        layer_N=args.layer_N,
        gru_layers=args.gru_layers,
        learning_rate=args.lr,
        device=args.device,
        output_dir=args.output_dir
    )

    # æ¢å¤è®­ç»ƒ
    if args.resume:
        pretrainer.load_model(args.resume, load_training_state=True)
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")

    # æ”¶é›†ç¤ºæ•™æ•°æ®
    demo_files = args.demo_files or []
    if args.collect_demo:
        demo_file = pretrainer.collect_extended_demo_data(
            strategies=args.strategies,
            steps_per_strategy=args.steps_per_strategy
        )
        demo_files.append(demo_file)

    if not demo_files:
        print("âŒ é”™è¯¯: æ²¡æœ‰æŒ‡å®šç¤ºæ•™æ•°æ®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ --demo-files æˆ– --collect-demo")
        return

    # æ‰§è¡Œè®­ç»ƒ
    history = pretrainer.train(
        demo_files=demo_files,
        epochs=args.epochs,
        batch_size=args.batch_size
    )

    print("ğŸ‰ ç‹¬ç«‹é¢„è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()