#!/usr/bin/env python3
"""
æ™ºèƒ½CSVä¿®å¤å·¥å…·

åˆ†æå®é™…çš„PPOæ•°æ®è¡Œï¼Œæ¨æ–­å‡ºæ­£ç¡®çš„åˆ—åå’Œæ•°æ®ç»“æ„ï¼Œ
ç„¶åé‡å»ºå®Œæ•´çš„CSVæ–‡ä»¶ã€‚
"""

import csv
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple


def extract_ppo_data_sample(csv_path: str) -> Tuple[List[str], List[List[str]]]:
    """
    æå–PPOæ•°æ®æ ·æœ¬

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„

    Returns:
        (å¤´éƒ¨åˆ—è¡¨, PPOæ•°æ®è¡Œåˆ—è¡¨)
    """
    print(f"ğŸ” æå–PPOæ•°æ®æ ·æœ¬...")

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)

        # æŸ¥æ‰¾PPOæ›´æ–°è¡Œ
        ppo_rows = []
        for line_num, row in enumerate(reader, start=2):
            # åœ¨è¡Œä¸­æŸ¥æ‰¾'ppo_update'æ–‡æœ¬
            if 'ppo_update' in row:
                ppo_rows.append(row)
                if len(ppo_rows) >= 3:  # å–å‰3ä¸ªæ ·æœ¬å°±å¤Ÿäº†
                    break

    print(f"ğŸ“Š æ‰¾åˆ° {len(ppo_rows)} ä¸ªPPOæ•°æ®æ ·æœ¬")
    return header, ppo_rows


def infer_missing_columns(header: List[str], ppo_sample: List[str]) -> List[str]:
    """
    ä»PPOæ•°æ®æ ·æœ¬æ¨æ–­ç¼ºå¤±çš„åˆ—å

    Args:
        header: ç°æœ‰å¤´éƒ¨
        ppo_sample: PPOæ•°æ®æ ·æœ¬è¡Œ

    Returns:
        æ¨æ–­çš„ç¼ºå¤±åˆ—å
    """
    # PPOæŒ‡æ ‡åœ¨MetricsExporterä¸­çš„æ ‡å‡†é¡ºåº
    standard_ppo_fields = [
        'pi_loss', 'vf_loss', 'entropy', 'approx_kl',
        'clipfrac', 'pg_grad_norm', 'explained_var', 'lr',
        'rollout_length', 'buffer_size'
    ]

    missing_fields = []
    for field in standard_ppo_fields:
        if field not in header:
            missing_fields.append(field)

    extra_columns_needed = len(ppo_sample) - len(header)
    print(f"ğŸ“ PPOè¡Œé•¿åº¦: {len(ppo_sample)}, å¤´éƒ¨é•¿åº¦: {len(header)}")
    print(f"ğŸ“ˆ éœ€è¦é¢å¤–åˆ—æ•°: {extra_columns_needed}")
    print(f"ğŸ¯ æ ‡å‡†ç¼ºå¤±å­—æ®µ: {missing_fields}")

    # å¦‚æœç¼ºå¤±å­—æ®µæ•°ä¸å¤Ÿï¼Œæ·»åŠ é€šç”¨å­—æ®µ
    if len(missing_fields) < extra_columns_needed:
        for i in range(len(missing_fields), extra_columns_needed):
            missing_fields.append(f"unknown_field_{i+1}")

    return missing_fields[:extra_columns_needed]


def analyze_ppo_values(header: List[str], ppo_rows: List[List[str]]) -> Dict[str, List[float]]:
    """
    åˆ†æPPOå€¼çš„åˆ†å¸ƒï¼Œå¸®åŠ©ç¡®è®¤å­—æ®µæ˜ å°„

    Args:
        header: å®Œæ•´å¤´éƒ¨
        ppo_rows: PPOæ•°æ®è¡Œ

    Returns:
        å­—æ®µååˆ°å€¼åˆ—è¡¨çš„æ˜ å°„
    """
    field_values = {}

    # æ‰¾åˆ°å¯èƒ½åŒ…å«PPOæŒ‡æ ‡çš„åˆ—
    potential_ppo_start = len(header) - 10  # å‡è®¾PPOå­—æ®µåœ¨æœ«å°¾

    ppo_field_names = [
        'pi_loss', 'vf_loss', 'entropy', 'approx_kl',
        'clipfrac', 'pg_grad_norm', 'explained_var', 'lr',
        'rollout_length', 'buffer_size'
    ]

    for i, field_name in enumerate(ppo_field_names):
        col_idx = potential_ppo_start + i
        if col_idx < len(header):
            values = []
            for row in ppo_rows:
                if col_idx < len(row):
                    try:
                        val = float(row[col_idx]) if row[col_idx] else 0.0
                        values.append(val)
                    except ValueError:
                        values.append(0.0)
            field_values[field_name] = values

    return field_values


def rebuild_csv_with_proper_headers(csv_path: str, output_path: str) -> bool:
    """
    é‡å»ºCSVæ–‡ä»¶ï¼Œä½¿ç”¨æ­£ç¡®çš„å¤´éƒ¨

    Args:
        csv_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        # æå–æ ·æœ¬æ•°æ®
        header, ppo_samples = extract_ppo_data_sample(csv_path)
        if not ppo_samples:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°PPOæ•°æ®æ ·æœ¬")
            return False

        # æ¨æ–­ç¼ºå¤±åˆ—
        missing_columns = infer_missing_columns(header, ppo_samples[0])
        new_header = header + missing_columns

        print(f"ğŸ“Š åŸå§‹å¤´éƒ¨åˆ—æ•°: {len(header)}")
        print(f"ğŸ“ˆ æ–°å¤´éƒ¨åˆ—æ•°: {len(new_header)}")
        print(f"ğŸ†• æ–°å¢åˆ—: {missing_columns}")

        # é‡å»ºæ–‡ä»¶
        with open(csv_path, 'r', encoding='utf-8') as infile:
            reader = csv.reader(infile)
            original_header = next(reader)  # è·³è¿‡åŸå§‹å¤´éƒ¨

            with open(output_path, 'w', newline='', encoding='utf-8') as outfile:
                writer = csv.writer(outfile)

                # å†™å…¥æ–°å¤´éƒ¨
                writer.writerow(new_header)

                # å†™å…¥æ•°æ®è¡Œï¼Œç¡®ä¿æ¯è¡Œéƒ½æœ‰æ­£ç¡®çš„åˆ—æ•°
                for row in reader:
                    # æˆªæ–­æˆ–å¡«å……åˆ°æ­£ç¡®é•¿åº¦
                    if len(row) > len(new_header):
                        row = row[:len(new_header)]
                    elif len(row) < len(new_header):
                        row = row + [''] * (len(new_header) - len(row))

                    writer.writerow(row)

        print(f"âœ… é‡å»ºå®Œæˆ: {output_path}")

        # éªŒè¯ç»“æœ
        return verify_rebuilt_csv(output_path, missing_columns)

    except Exception as e:
        print(f"âŒ é‡å»ºå¤±è´¥: {e}")
        return False


def verify_rebuilt_csv(csv_path: str, expected_new_columns: List[str]) -> bool:
    """
    éªŒè¯é‡å»ºçš„CSVæ–‡ä»¶

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        expected_new_columns: æœŸæœ›çš„æ–°åˆ—

    Returns:
        éªŒè¯æ˜¯å¦æˆåŠŸ
    """
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)

            # æ£€æŸ¥å‡ è¡Œæ•°æ®çš„ä¸€è‡´æ€§
            inconsistent_rows = 0
            for i, row in enumerate(reader):
                if len(row) != len(header):
                    inconsistent_rows += 1
                if i >= 100:  # åªæ£€æŸ¥å‰100è¡Œ
                    break

        if inconsistent_rows > 0:
            print(f"âš ï¸  å‘ç° {inconsistent_rows} è¡Œé•¿åº¦ä¸ä¸€è‡´")
            return False

        # æ£€æŸ¥æ–°åˆ—æ˜¯å¦å­˜åœ¨
        missing = [col for col in expected_new_columns if col not in header]
        if missing:
            print(f"âš ï¸  ç¼ºå¤±æœŸæœ›åˆ—: {missing}")
            return False

        print("âœ… CSVæ–‡ä»¶éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False


def show_ppo_preview(csv_path: str) -> None:
    """
    æ˜¾ç¤ºä¿®å¤åPPOæ•°æ®çš„é¢„è§ˆ

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
    """
    try:
        import pandas as pd

        df = pd.read_csv(csv_path)
        ppo_data = df[df['data_type'] == 'ppo_update']

        if len(ppo_data) == 0:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°PPOæ›´æ–°æ•°æ®")
            return

        print(f"\nğŸ“Š PPOæ•°æ®é¢„è§ˆ (å…± {len(ppo_data)} è¡Œ):")
        print("=" * 60)

        # æ˜¾ç¤ºå…³é”®PPOæŒ‡æ ‡
        ppo_columns = ['step', 'pi_loss', 'vf_loss', 'entropy', 'approx_kl', 'lr']
        available_columns = [col for col in ppo_columns if col in ppo_data.columns]

        if available_columns:
            preview = ppo_data[available_columns].head(5)
            print(preview.to_string(index=False))
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ ‡å‡†PPOåˆ—")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if 'entropy' in ppo_data.columns:
            entropy_values = ppo_data['entropy'].dropna()
            if len(entropy_values) > 0:
                print(f"\nğŸ² ç†µç»Ÿè®¡: å¹³å‡={entropy_values.mean():.4f}, èŒƒå›´=[{entropy_values.min():.4f}, {entropy_values.max():.4f}]")

    except ImportError:
        print("âš ï¸  éœ€è¦pandasæ¥æ˜¾ç¤ºé¢„è§ˆ")
    except Exception as e:
        print(f"âš ï¸  é¢„è§ˆå¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='æ™ºèƒ½ä¿®å¤æŸåçš„PPO CSVæ–‡ä»¶')
    parser.add_argument('input_csv', help='è¾“å…¥CSVæ–‡ä»¶')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--preview', action='store_true', help='æ˜¾ç¤ºä¿®å¤åçš„PPOæ•°æ®é¢„è§ˆ')

    args = parser.parse_args()

    if not Path(args.input_csv).exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_csv}")
        return 1

    print("ğŸ”§ å¼€å§‹æ™ºèƒ½ä¿®å¤CSVæ–‡ä»¶...")
    success = rebuild_csv_with_proper_headers(args.input_csv, args.output)

    if success:
        print("\nğŸ‰ ä¿®å¤æˆåŠŸ!")

        if args.preview:
            show_ppo_preview(args.output)

        return 0
    else:
        print("\nğŸ’¥ ä¿®å¤å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())