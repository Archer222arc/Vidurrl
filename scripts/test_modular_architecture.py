#!/usr/bin/env python3
"""
Test Modular Architecture Components

Verify that the new modular temporal LSTM component works correctly
and maintains compatibility with existing checkpoints.
"""

import sys
import torch
import json
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.models.actor_critic import ActorCritic
from src.core.models.components import TemporalLSTM, TemporalLSTMConfig, TemporalLSTMFactory


def test_temporal_lstm_component():
    """Test the modular temporal LSTM component."""
    print("üß™ Testing Temporal LSTM Component...")

    # Test configuration
    hidden_size = 320
    batch_size = 4

    # Test disabled component
    config_disabled = TemporalLSTMConfig(enable=False)
    lstm_disabled = TemporalLSTM(hidden_size, config_disabled)

    print(f"‚úÖ Disabled component created: {lstm_disabled.get_component_info()}")

    # Test enabled component
    config_enabled = TemporalLSTMConfig(
        enable=True,
        feature_chunks=4,
        hidden_size_ratio=0.25,
        bidirectional=True,
        residual_connections=True
    )
    lstm_enabled = TemporalLSTM(hidden_size, config_enabled)

    print(f"‚úÖ Enabled component created: {lstm_enabled.get_component_info()}")

    # Test forward pass
    x = torch.randn(batch_size, hidden_size)

    # Disabled component should return input unchanged
    out_disabled = lstm_disabled(x)
    assert torch.equal(x, out_disabled), "Disabled component should return input unchanged"
    print("‚úÖ Disabled component passes input unchanged")

    # Enabled component should process input
    out_enabled = lstm_enabled(x)
    assert out_enabled.shape == x.shape, "Output shape should match input shape"
    assert not torch.equal(x, out_enabled), "Enabled component should modify input"
    print("‚úÖ Enabled component processes input correctly")

    return True


def test_factory_creation():
    """Test factory-based component creation."""
    print("\nüè≠ Testing Factory Creation...")

    hidden_size = 320

    # Test creation from dictionary
    config_dict = {
        "enable": True,
        "feature_chunks": 4,
        "hidden_size_ratio": 0.25,
        "bidirectional": True,
        "residual_connections": True
    }

    lstm_from_dict = TemporalLSTMFactory.from_dict(config_dict, hidden_size)
    print(f"‚úÖ Created from dict: {lstm_from_dict.get_component_info()}")

    # Test creation from PPO config mock
    class MockPPOConfig:
        enable_temporal_lstm = True
        temporal_lstm_feature_chunks = 4
        temporal_lstm_hidden_ratio = 0.25
        temporal_lstm_bidirectional = True

    mock_config = MockPPOConfig()
    lstm_from_ppo = TemporalLSTMFactory.from_ppo_config(mock_config, hidden_size)
    print(f"‚úÖ Created from PPO config: {lstm_from_ppo.get_component_info()}")

    return True


def test_actor_critic_integration():
    """Test integration with ActorCritic."""
    print("\nüé≠ Testing ActorCritic Integration...")

    # Model configuration
    state_dim = 322
    action_dim = 4
    hidden_size = 320

    # Test with decoupled=False (should disable temporal LSTM)
    ac_disabled = ActorCritic(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_size=hidden_size,
        enable_decoupled=False
    )

    print(f"‚úÖ ActorCritic with decoupled=False: temporal_lstm={ac_disabled.enable_temporal_lstm}")

    # Test with decoupled=True (should enable temporal LSTM)
    ac_enabled = ActorCritic(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_size=hidden_size,
        enable_decoupled=True
    )

    print(f"‚úÖ ActorCritic with decoupled=True: temporal_lstm={ac_enabled.enable_temporal_lstm}")

    # Test forward pass
    batch_size = 2
    x = torch.randn(batch_size, state_dim)

    with torch.no_grad():
        logits_disabled, value_disabled, _ = ac_disabled(x)
        logits_enabled, value_enabled, _ = ac_enabled(x)

    print(f"‚úÖ Forward pass shapes - Disabled: {logits_disabled.shape}, {value_disabled.shape}")
    print(f"‚úÖ Forward pass shapes - Enabled: {logits_enabled.shape}, {value_enabled.shape}")

    # Get component information
    if ac_enabled.enable_temporal_lstm:
        component_info = ac_enabled.temporal_lstm_component.get_component_info()
        print(f"‚úÖ Temporal LSTM component info: {component_info}")

    return True


def test_checkpoint_compatibility():
    """Test compatibility with existing checkpoints."""
    print("\nüíæ Testing Checkpoint Compatibility...")

    checkpoint_path = "/Users/ruicheng/Documents/GitHub/Vidur/Vidur_arc2/outputs/checkpoints/latest.pt"

    try:
        # Load checkpoint
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model_config = checkpoint['model_config']
        state_dict = checkpoint['model_state_dict']

        print(f"‚úÖ Loaded checkpoint with config: {model_config}")

        # Check for temporal layers in checkpoint
        temporal_keys = [k for k in state_dict.keys() if 'temporal' in k]
        print(f"‚úÖ Temporal keys in checkpoint: {temporal_keys}")

        # Create new model with same config
        new_model = ActorCritic(
            state_dim=model_config['state_dim'],
            action_dim=model_config['action_dim'],
            hidden_size=model_config['hidden_size'],
            layer_N=model_config['layer_N'],
            gru_layers=model_config['gru_layers'],
            enable_decoupled=model_config['enable_decoupled'],
            feature_projection_dim=model_config.get('feature_projection_dim')
        )

        # Check model structure
        model_keys = set(new_model.state_dict().keys())
        checkpoint_keys = set(state_dict.keys())

        common_keys = model_keys & checkpoint_keys
        model_only = model_keys - checkpoint_keys
        checkpoint_only = checkpoint_keys - model_keys

        print(f"‚úÖ Parameter compatibility:")
        print(f"   - Common parameters: {len(common_keys)}")
        print(f"   - Model-only parameters: {len(model_only)}")
        print(f"   - Checkpoint-only parameters: {len(checkpoint_only)}")

        if model_only:
            print(f"   - Model-only keys: {sorted(model_only)}")
        if checkpoint_only:
            print(f"   - Checkpoint-only keys: {sorted(checkpoint_only)}")

        # Try loading with strict=False
        try:
            missing_keys, unexpected_keys = new_model.load_state_dict(state_dict, strict=False)
            print(f"‚úÖ Loaded checkpoint with strict=False:")
            print(f"   - Missing keys: {len(missing_keys)}")
            print(f"   - Unexpected keys: {len(unexpected_keys)}")

            if len(unexpected_keys) <= 5:  # Allow some differences
                print("‚úÖ Checkpoint compatibility: GOOD")
            else:
                print("‚ö†Ô∏è  Checkpoint compatibility: MODERATE")

        except Exception as e:
            print(f"‚ùå Failed to load checkpoint: {e}")
            return False

    except Exception as e:
        print(f"‚ö†Ô∏è  Could not test checkpoint compatibility: {e}")
        return True  # Don't fail if no checkpoint available

    return True


def test_configuration_control():
    """Test configuration-based control of temporal LSTM."""
    print("\n‚öôÔ∏è  Testing Configuration Control...")

    # Load configuration file
    config_path = "configs/ppo_warmstart.json"

    try:
        with open(config_path, 'r') as f:
            config = json.load(f)

        temporal_config = config.get("actor_critic_architecture", {}).get("temporal_lstm", {})
        print(f"‚úÖ Loaded temporal config: {temporal_config}")

        # Test that config can control component creation
        hidden_size = 320
        lstm_component = TemporalLSTMFactory.from_dict(temporal_config, hidden_size)

        component_info = lstm_component.get_component_info()
        print(f"‚úÖ Component created from config: {component_info}")

        # Verify configuration values match
        expected_enable = temporal_config.get("enable", False)
        actual_enable = component_info["enabled"]

        if expected_enable == actual_enable:
            print(f"‚úÖ Configuration control working: enable={actual_enable}")
        else:
            print(f"‚ùå Configuration mismatch: expected={expected_enable}, actual={actual_enable}")
            return False

    except Exception as e:
        print(f"‚ö†Ô∏è  Could not test configuration control: {e}")
        return True  # Don't fail if config not available

    return True


def main():
    """Run all tests."""
    print("üöÄ Testing Modular Architecture Components")
    print("=" * 60)

    tests = [
        ("Temporal LSTM Component", test_temporal_lstm_component),
        ("Factory Creation", test_factory_creation),
        ("ActorCritic Integration", test_actor_critic_integration),
        ("Checkpoint Compatibility", test_checkpoint_compatibility),
        ("Configuration Control", test_configuration_control),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            print(f"\nüìã Running: {test_name}")
            result = test_func()
            results.append((test_name, result))
            print(f"{'‚úÖ' if result else '‚ùå'} {test_name}: {'PASSED' if result else 'FAILED'}")
        except Exception as e:
            results.append((test_name, False))
            print(f"‚ùå {test_name}: FAILED with exception: {e}")

    # Summary
    print("\n" + "=" * 60)
    print("üìä Test Results Summary:")
    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"   {test_name}: {status}")

    print(f"\nüéØ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")

    if passed == total:
        print("üéâ All tests passed! Modular architecture is working correctly.")
        return 0
    else:
        print("‚ö†Ô∏è  Some tests failed. Please review the implementation.")
        return 1


if __name__ == "__main__":
    exit(main())