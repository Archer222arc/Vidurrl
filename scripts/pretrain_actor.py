#!/usr/bin/env python3
"""
Actoré¢„è®­ç»ƒè„šæœ¬ - ç»Ÿä¸€æ¥å£

ä½¿ç”¨ç»Ÿä¸€é¢„è®­ç»ƒç®¡ç†å™¨è¿›è¡ŒActoré¢„è®­ç»ƒï¼Œ
å…¼å®¹åŸå§‹æ¥å£çš„åŒæ—¶æä¾›æ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚
"""

import argparse
import sys
from pathlib import Path


def _bootstrap_repo_path() -> None:
    """Ensure imports resolve to the current repo checkout."""
    repo_root = Path(__file__).resolve().parent.parent
    conflict_root = repo_root.parent / "Vidur"

    def _same_path(a: str, b: Path) -> bool:
        try:
            return Path(a).resolve() == b.resolve()
        except (OSError, RuntimeError):
            return False

    sys.path[:] = [p for p in sys.path if not _same_path(p, conflict_root)]

    repo_root_str = str(repo_root)
    if repo_root_str not in sys.path:
        sys.path.insert(0, repo_root_str)


_bootstrap_repo_path()

from src.pretraining.unified_trainer import UnifiedPretrainer


def main():
    """å…¼å®¹åŸå§‹æ¥å£çš„ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Actoré¢„è®­ç»ƒ - è¡Œä¸ºå…‹éš†")
    parser.add_argument("--demo", type=str, required=True, help="ç¤ºæ•™æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=256, help="æ‰¹å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--hidden_size", type=int, default=128, help="éšè—å±‚å¤§å°")
    parser.add_argument("--layer_N", type=int, default=2, help="MLPå±‚æ•°")
    parser.add_argument("--gru_layers", type=int, default=2, help="GRUå±‚æ•°")
    parser.add_argument("--output", type=str, default="./outputs/pretrained_actor.pt",
                       help="è¾“å‡ºæ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", type=str, default="cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--resume", type=str, help="ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹ (å¾®è°ƒæ¨¡å¼)")

    args = parser.parse_args()

    # ä»ç¤ºæ•™æ–‡ä»¶ä¸­æ¨æ–­é…ç½®
    import pickle
    with open(args.demo, 'rb') as f:
        data = pickle.load(f)

    stats = data.get('stats', {})
    state_dim = stats.get('state_dim', 64)  # é»˜è®¤å€¼
    action_dim = stats.get('action_dim', 4)  # é»˜è®¤å€¼

    # åˆ›å»ºé…ç½®
    config = {
        'training_mode': 'standard',
        'state_dim': state_dim,
        'action_dim': action_dim,
        'hidden_size': args.hidden_size,
        'layer_N': args.layer_N,
        'gru_layers': args.gru_layers,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.lr,
        'validation_split': 0.1,
        'device': args.device,
        'augment_data': False,
        'output_dir': str(Path(args.output).parent),
    }

    print(f"ğŸ“„ é…ç½®ä¿¡æ¯:")
    print(f"   - ç¤ºæ•™æ–‡ä»¶: {args.demo}")
    print(f"   - çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"   - è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"   - è¾“å‡ºè·¯å¾„: {args.output}")

    # åˆ›å»ºç»Ÿä¸€é¢„è®­ç»ƒå™¨
    pretrainer = UnifiedPretrainer(config)

    try:
        if args.resume:
            # å¾®è°ƒæ¨¡å¼
            print(f"ğŸ”§ å¾®è°ƒæ¨¡å¼: åŸºäº {args.resume}")
            output_path = pretrainer.load_and_fine_tune(
                base_model_path=args.resume,
                demo_files=[args.demo],
                output_filename=Path(args.output).name,
                fine_tune_epochs=args.epochs
            )
        else:
            # æ ‡å‡†è®­ç»ƒæ¨¡å¼
            print(f"ğŸ¤– æ ‡å‡†è®­ç»ƒæ¨¡å¼")
            output_path = pretrainer.train_from_demo_files(
                demo_files=[args.demo],
                output_filename=Path(args.output).name
            )

        print(f"ğŸ‰ é¢„è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“‚ è¾“å‡ºæ¨¡å‹: {output_path}")

        # éªŒè¯æ¨¡å‹
        if pretrainer.validate_model(output_path):
            print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡")
        else:
            print("âŒ æ¨¡å‹éªŒè¯å¤±è´¥")

    except Exception as e:
        print(f"âŒ é¢„è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()