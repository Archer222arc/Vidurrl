#!/usr/bin/env python3
"""
Actoré¢„è®­ç»ƒè„šæœ¬ - è¡Œä¸ºå…‹éš†

ä½¿ç”¨æ”¶é›†çš„ç¤ºæ•™æ•°æ®å¯¹PPOçš„Actorè¿›è¡Œé¢„è®­ç»ƒï¼Œ
ä¸ºåç»­çš„PPOè®­ç»ƒæä¾›è‰¯å¥½çš„åˆå§‹ç­–ç•¥ã€‚
"""

import argparse
import pickle
import sys
from pathlib import Path
from typing import Dict, Any, List
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader


def _bootstrap_repo_path() -> None:
    """Ensure imports resolve to the current repo checkout."""
    repo_root = Path(__file__).resolve().parent.parent
    conflict_root = repo_root.parent / "Vidur"

    def _same_path(a: str, b: Path) -> bool:
        try:
            return Path(a).resolve() == b.resolve()
        except (OSError, RuntimeError):
            return False

    sys.path[:] = [p for p in sys.path if not _same_path(p, conflict_root)]

    repo_root_str = str(repo_root)
    if repo_root_str not in sys.path:
        sys.path.insert(0, repo_root_str)


_bootstrap_repo_path()

from src.rl_components.actor_critic import ActorCritic
from src.rl_components.normalizers import RunningNormalizer


class DemoDataset(Dataset):
    """ç¤ºæ•™æ•°æ®é›†"""

    def __init__(self, demo_data: List[Dict[str, Any]], normalizer: RunningNormalizer = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            demo_data: ç¤ºæ•™æ•°æ®åˆ—è¡¨
            normalizer: çŠ¶æ€å½’ä¸€åŒ–å™¨
        """
        self.states = []
        self.actions = []

        for item in demo_data:
            state = np.array(item['state'], dtype=np.float32)
            action = int(item['action'])

            # åº”ç”¨å½’ä¸€åŒ–
            if normalizer:
                state = normalizer.normalize(state)

            self.states.append(state)
            self.actions.append(action)

        self.states = np.array(self.states)
        self.actions = np.array(self.actions)

        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(self.states)}")
        print(f"ğŸ¯ çŠ¶æ€ç»´åº¦: {self.states.shape[1] if len(self.states) > 0 else 0}")
        print(f"ğŸ“ˆ åŠ¨ä½œåˆ†å¸ƒ: {np.bincount(self.actions)}")

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        return torch.from_numpy(self.states[idx]), torch.tensor(self.actions[idx], dtype=torch.long)


class BehaviorCloningTrainer:
    """è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int = 4,
        hidden_size: int = 128,
        layer_N: int = 2,
        gru_layers: int = 2,
        enable_decoupled: bool = True,
        feature_projection_dim: int = None,
        device: str = "cpu"
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            hidden_size: éšè—å±‚å¤§å°
            layer_N: MLPå±‚æ•°
            gru_layers: GRUå±‚æ•°
            enable_decoupled: æ˜¯å¦ä½¿ç”¨è§£è€¦æ¶æ„
            feature_projection_dim: ç‰¹å¾æŠ•å½±ç»´åº¦
            device: è®¾å¤‡
        """
        self.device = device
        self.action_dim = action_dim

        # åˆ›å»ºActor-Criticç½‘ç»œï¼ˆåªè®­ç»ƒActoréƒ¨åˆ†ï¼‰
        self.actor_critic = ActorCritic(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_size=hidden_size,
            layer_N=layer_N,
            gru_layers=gru_layers,
            enable_decoupled=enable_decoupled,
            feature_projection_dim=feature_projection_dim,
        ).to(device)

        # åªä¼˜åŒ–Actorå‚æ•°
        actor_params = []
        if hasattr(self.actor_critic, 'actor_branch') and self.actor_critic.enable_decoupled:
            # è§£è€¦æ¶æ„ï¼šActorå‚æ•°åœ¨actor_branchå’Œactor_headä¸­
            actor_params.extend(self.actor_critic.actor_branch.parameters())
            actor_params.extend(self.actor_critic.actor_head.parameters())
        elif hasattr(self.actor_critic, 'actor'):
            # è€¦åˆæ¶æ„ï¼šActorå‚æ•°åœ¨actorä¸­
            actor_params.extend(self.actor_critic.actor.parameters())
        else:
            raise ValueError("æ— æ³•æ‰¾åˆ°Actorå‚æ•°ï¼ŒActorCriticæ¶æ„ä¸åŒ¹é…")

        self.optimizer = torch.optim.Adam(actor_params, lr=1e-3)

        print(f"ğŸ¤– Actor-Criticç½‘ç»œå·²åˆ›å»º: state_dim={state_dim}, action_dim={action_dim}")

    def train(
        self,
        dataset: DemoDataset,
        epochs: int = 10,
        batch_size: int = 256,
        validation_split: float = 0.1
    ) -> Dict[str, List[float]]:
        """
        è®­ç»ƒActorç½‘ç»œ

        Args:
            dataset: ç¤ºæ•™æ•°æ®é›†
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            validation_split: éªŒè¯é›†æ¯”ä¾‹

        Returns:
            è®­ç»ƒå†å²
        """
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        total_size = len(dataset)
        val_size = int(total_size * validation_split)
        train_size = total_size - val_size

        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

        print(f"ğŸš€ å¼€å§‹è¡Œä¸ºå…‹éš†è®­ç»ƒ: epochs={epochs}, batch_size={batch_size}")
        print(f"ğŸ“Š è®­ç»ƒé›†: {train_size}, éªŒè¯é›†: {val_size}")

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.actor_critic.train()
            train_loss, train_acc = self._train_epoch(train_loader)

            # éªŒè¯é˜¶æ®µ
            self.actor_critic.eval()
            val_loss, val_acc = self._validate_epoch(val_loader)

            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)

            print(f"Epoch {epoch+1}/{epochs}: "
                  f"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, "
                  f"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}")

        print("âœ… è¡Œä¸ºå…‹éš†è®­ç»ƒå®Œæˆ!")
        return history

    def _train_epoch(self, dataloader: DataLoader) -> tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        total_loss = 0.0
        correct = 0
        total = 0

        for states, actions in dataloader:
            states = states.to(self.device)
            actions = actions.to(self.device)

            # å‰å‘ä¼ æ’­ï¼ˆåªä½¿ç”¨Actorï¼‰
            batch_size = states.shape[0]
            hidden_state = torch.zeros(
                self.actor_critic.gru_layers, batch_size, self.actor_critic.hidden_size,
                device=self.device
            )
            mask = torch.ones(batch_size, 1, device=self.device)

            # é€šè¿‡ç½‘ç»œè·å–logits
            z = self.actor_critic.forward_mlp(states)
            z, _ = self.actor_critic.forward_gru(z, hidden_state, mask)

            # æ ¹æ®æ¶æ„ç±»å‹è·å–logits
            if hasattr(self.actor_critic, 'actor_branch') and self.actor_critic.enable_decoupled:
                z_actor = self.actor_critic.actor_branch(z)
                logits = self.actor_critic.actor_head(z_actor)
            else:
                logits = self.actor_critic.actor(z)

            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(logits, actions)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            correct += (predicted == actions).sum().item()
            total += actions.size(0)

        return total_loss / len(dataloader), correct / total

    def _validate_epoch(self, dataloader: DataLoader) -> tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for states, actions in dataloader:
                states = states.to(self.device)
                actions = actions.to(self.device)

                # å‰å‘ä¼ æ’­
                batch_size = states.shape[0]
                hidden_state = torch.zeros(
                    self.actor_critic.gru_layers, batch_size, self.actor_critic.hidden_size,
                    device=self.device
                )
                mask = torch.ones(batch_size, 1, device=self.device)

                z = self.actor_critic.forward_mlp(states)
                z, _ = self.actor_critic.forward_gru(z, hidden_state, mask)

                # æ ¹æ®æ¶æ„ç±»å‹è·å–logits
                if hasattr(self.actor_critic, 'actor_branch') and self.actor_critic.enable_decoupled:
                    z_actor = self.actor_critic.actor_branch(z)
                    logits = self.actor_critic.actor_head(z_actor)
                else:
                    logits = self.actor_critic.actor(z)

                # è®¡ç®—æŸå¤±
                loss = F.cross_entropy(logits, actions)

                # ç»Ÿè®¡
                total_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                correct += (predicted == actions).sum().item()
                total += actions.size(0)

        return total_loss / len(dataloader), correct / total

    def save_pretrained_model(self, output_path: str) -> None:
        """ä¿å­˜é¢„è®­ç»ƒçš„æ¨¡å‹"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        torch.save({
            'model_state_dict': self.actor_critic.state_dict(),
            'model_config': {
                'state_dim': self.actor_critic.state_dim,
                'action_dim': self.actor_critic.action_dim,
                'hidden_size': self.actor_critic.hidden_size,
                'layer_N': self.actor_critic.layer_N,
                'gru_layers': self.actor_critic.gru_layers,
                'enable_decoupled': getattr(self.actor_critic, 'enable_decoupled', False),
                'feature_projection_dim': getattr(self.actor_critic, 'feature_projection_dim', None),
            }
        }, output_path)

        print(f"ğŸ’¾ é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Actoré¢„è®­ç»ƒ - è¡Œä¸ºå…‹éš†")
    parser.add_argument("--demo", type=str, required=True, help="ç¤ºæ•™æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=256, help="æ‰¹å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--hidden_size", type=int, default=128, help="éšè—å±‚å¤§å°")
    parser.add_argument("--layer_N", type=int, default=2, help="MLPå±‚æ•°")
    parser.add_argument("--gru_layers", type=int, default=2, help="GRUå±‚æ•°")
    parser.add_argument("--output", type=str, default="./outputs/pretrained_actor.pt",
                       help="è¾“å‡ºæ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", type=str, default="cpu", help="è®­ç»ƒè®¾å¤‡")

    args = parser.parse_args()

    # åŠ è½½ç¤ºæ•™æ•°æ®
    print(f"ğŸ“‚ åŠ è½½ç¤ºæ•™æ•°æ®: {args.demo}")
    with open(args.demo, 'rb') as f:
        data = pickle.load(f)

    demo_data = data['demo_data']
    stats = data['stats']

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {stats}")

    # åˆ›å»ºæ•°æ®é›†ï¼ˆä¸ä½¿ç”¨å½’ä¸€åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼‰
    dataset = DemoDataset(demo_data)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BehaviorCloningTrainer(
        state_dim=stats['state_dim'],
        action_dim=len(stats['action_distribution']),
        hidden_size=args.hidden_size,
        layer_N=args.layer_N,
        gru_layers=args.gru_layers,
        device=args.device
    )

    # è®­ç»ƒ
    history = trainer.train(
        dataset=dataset,
        epochs=args.epochs,
        batch_size=args.batch_size
    )

    # ä¿å­˜æ¨¡å‹
    trainer.save_pretrained_model(args.output)

    print("ğŸ‰ é¢„è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
