#!/usr/bin/env python3
"""
è§£ææº¢å‡ºPPOæ•°æ®å·¥å…·

ä¸“é—¨ç”¨äºè§£æç”±äºCSVæ¶æ„æ¼”åŒ–é—®é¢˜å¯¼è‡´çš„"æº¢å‡º"PPOæ•°æ®ï¼Œ
ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­æå–å¹¶æ˜¾ç¤ºPPOè®­ç»ƒæŒ‡æ ‡ã€‚
"""

import csv
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple


class PPODataParser:
    """PPOæº¢å‡ºæ•°æ®è§£æå™¨"""

    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.header = []
        self.ppo_rows = []

    def load_data(self) -> bool:
        """åŠ è½½CSVæ•°æ®"""
        try:
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                self.header = next(reader)

                # æŸ¥æ‰¾PPOæ›´æ–°è¡Œ
                for row in reader:
                    if 'ppo_update' in row:
                        self.ppo_rows.append(row)

            print(f"ğŸ“Š åŠ è½½äº† {len(self.ppo_rows)} è¡ŒPPOæ•°æ®")
            return len(self.ppo_rows) > 0

        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False

    def parse_ppo_row(self, row: List[str]) -> Dict[str, float]:
        """
        è§£æå•ä¸ªPPOè¡Œï¼Œæ ¹æ®å®é™…è§‚å¯Ÿçš„æ•°æ®ç»“æ„

        æ ¹æ®åˆ†æï¼ŒPPOæ•°æ®åœ¨æº¢å‡ºåˆ—ä¸­çš„ä½ç½®ï¼š
        - æº¢å‡ºåˆ—1: approx_kl
        - æº¢å‡ºåˆ—2: buffer_size
        - æº¢å‡ºåˆ—3: clipfrac
        - æº¢å‡ºåˆ—6: entropy
        - æº¢å‡ºåˆ—7: explained_var
        - æº¢å‡ºåˆ—8: lr
        - æº¢å‡ºåˆ—9: pg_grad_norm
        - æº¢å‡ºåˆ—10: pi_loss
        - æº¢å‡ºåˆ—14: vf_loss
        """
        base_cols = len(self.header)
        overflow_data = row[base_cols:] if len(row) > base_cols else []

        ppo_metrics = {}

        # æ ¹æ®è§‚å¯Ÿåˆ°çš„æ¨¡å¼è§£æ
        try:
            if len(overflow_data) >= 1:
                ppo_metrics['approx_kl'] = float(overflow_data[0]) if overflow_data[0] else 0.0
            if len(overflow_data) >= 2:
                ppo_metrics['buffer_size'] = float(overflow_data[1]) if overflow_data[1] else 0.0
            if len(overflow_data) >= 3:
                ppo_metrics['clipfrac'] = float(overflow_data[2]) if overflow_data[2] else 0.0
            if len(overflow_data) >= 6:
                ppo_metrics['entropy'] = float(overflow_data[5]) if overflow_data[5] else 0.0
            if len(overflow_data) >= 7:
                ppo_metrics['explained_var'] = float(overflow_data[6]) if overflow_data[6] else 0.0
            if len(overflow_data) >= 8:
                ppo_metrics['lr'] = float(overflow_data[7]) if overflow_data[7] else 0.0
            if len(overflow_data) >= 9:
                ppo_metrics['pg_grad_norm'] = float(overflow_data[8]) if overflow_data[8] else 0.0
            if len(overflow_data) >= 10:
                ppo_metrics['pi_loss'] = float(overflow_data[9]) if overflow_data[9] else 0.0
            if len(overflow_data) >= 14:
                ppo_metrics['vf_loss'] = float(overflow_data[13]) if overflow_data[13] else 0.0

        except (ValueError, IndexError) as e:
            print(f"âš ï¸  è§£æè¡Œæ—¶å‡ºé”™: {e}")

        return ppo_metrics

    def get_step_from_row(self, row: List[str]) -> int:
        """ä»è¡Œä¸­æå–æ­¥éª¤å·"""
        # å‡è®¾stepåœ¨æ ‡å‡†åˆ—ä¸­
        if 'step' in self.header:
            step_idx = self.header.index('step')
            if step_idx < len(row):
                try:
                    return int(float(row[step_idx]))
                except (ValueError, IndexError):
                    pass
        return 0

    def analyze_ppo_data(self) -> Dict[str, Any]:
        """åˆ†æPPOæ•°æ®"""
        if not self.ppo_rows:
            return {}

        all_metrics = []
        steps = []

        for row in self.ppo_rows:
            metrics = self.parse_ppo_row(row)
            step = self.get_step_from_row(row)

            if metrics:
                all_metrics.append(metrics)
                steps.append(step)

        if not all_metrics:
            return {}

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        metric_names = ['pi_loss', 'vf_loss', 'entropy', 'approx_kl', 'clipfrac',
                       'pg_grad_norm', 'explained_var', 'lr']

        for metric in metric_names:
            values = [m.get(metric, 0.0) for m in all_metrics if metric in m]
            if values:
                stats[metric] = {
                    'count': len(values),
                    'mean': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'latest': values[-1] if values else 0.0
                }

        return {
            'stats': stats,
            'total_updates': len(all_metrics),
            'steps': steps,
            'latest_metrics': all_metrics[-1] if all_metrics else {},
            'first_step': min(steps) if steps else 0,
            'last_step': max(steps) if steps else 0
        }

    def display_summary(self) -> None:
        """æ˜¾ç¤ºPPOè®­ç»ƒæ‘˜è¦"""
        analysis = self.analyze_ppo_data()

        if not analysis:
            print("âŒ æ²¡æœ‰å¯åˆ†æçš„PPOæ•°æ®")
            return

        print("ğŸš€ PPOè®­ç»ƒæ•°æ®è§£æç»“æœ")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"ğŸ“Š PPOæ›´æ–°æ¬¡æ•°: {analysis['total_updates']}")
        print(f"ğŸ“ˆ è®­ç»ƒæ­¥éª¤èŒƒå›´: {analysis['first_step']} - {analysis['last_step']}")
        print(f"â° æœ€æ–°æ­¥éª¤: {analysis['last_step']}")

        print("\nğŸ“Š PPOæŒ‡æ ‡ç»Ÿè®¡:")
        print("-" * 50)

        stats = analysis['stats']
        for metric in ['pi_loss', 'vf_loss', 'entropy', 'approx_kl', 'clipfrac', 'pg_grad_norm', 'explained_var']:
            if metric in stats:
                s = stats[metric]
                print(f"{metric:15s}: å¹³å‡={s['mean']:8.6f}, æœ€æ–°={s['latest']:8.6f}, èŒƒå›´=[{s['min']:6.4f}, {s['max']:6.4f}]")
            else:
                print(f"{metric:15s}: æ— æ•°æ®")

        # æ˜¾ç¤ºæœ€æ–°æŒ‡æ ‡
        latest = analysis['latest_metrics']
        if latest:
            print(f"\nğŸ¯ æœ€æ–°æŒ‡æ ‡è¯¦æƒ…:")
            print("-" * 30)
            for key, value in latest.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.6f}")

        # çŠ¶æ€è¯„ä¼°
        print(f"\nğŸ¯ è®­ç»ƒçŠ¶æ€è¯„ä¼°:")
        print("-" * 30)

        if 'entropy' in latest:
            entropy = latest['entropy']
            if entropy > 1.0:
                print("  ğŸ² æ¢ç´¢å……åˆ† (ç†µå€¼é«˜)")
            elif entropy > 0.5:
                print("  âš–ï¸  æ¢ç´¢é€‚ä¸­")
            else:
                print("  ğŸ¯ ç­–ç•¥æ”¶æ•› (ç†µå€¼ä½)")

        if 'pi_loss' in stats and stats['pi_loss']['count'] >= 3:
            recent_pi_loss = stats['pi_loss']['latest']
            if recent_pi_loss < -0.01:
                print("  âœ… ç­–ç•¥æŸå¤±è¾ƒä½")
            elif recent_pi_loss > 0.01:
                print("  âš ï¸  ç­–ç•¥æŸå¤±è¾ƒé«˜")
            else:
                print("  ğŸ“Š ç­–ç•¥æŸå¤±æ­£å¸¸")

        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    def export_clean_data(self, output_path: str) -> bool:
        """å¯¼å‡ºæ¸…ç†åçš„PPOæ•°æ®"""
        try:
            analysis = self.analyze_ppo_data()
            if not analysis:
                return False

            # åˆ›å»ºæ¸…ç†åçš„æ•°æ®
            clean_data = []
            for i, row in enumerate(self.ppo_rows):
                metrics = self.parse_ppo_row(row)
                step = self.get_step_from_row(row)

                # è·å–æ—¶é—´æˆ³ä¿¡æ¯
                timestamp = ""
                if 'datetime' in self.header:
                    dt_idx = self.header.index('datetime')
                    if dt_idx < len(row):
                        timestamp = row[dt_idx]

                clean_row = {
                    'step': step,
                    'datetime': timestamp,
                    'data_type': 'ppo_update',
                    **metrics
                }
                clean_data.append(clean_row)

            # å†™å…¥CSV
            if clean_data:
                fieldnames = ['step', 'datetime', 'data_type'] + list(clean_data[0].keys())[3:]
                with open(output_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(clean_data)

                print(f"ğŸ“ å·²å¯¼å‡ºæ¸…ç†æ•°æ®: {output_path}")
                print(f"ğŸ“Š å¯¼å‡º {len(clean_data)} è¡ŒPPOæ•°æ®")
                return True

        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

        return False


def main():
    parser = argparse.ArgumentParser(description='è§£ææº¢å‡ºçš„PPO CSVæ•°æ®')
    parser.add_argument('csv_file', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--export', '-e', help='å¯¼å‡ºæ¸…ç†åæ•°æ®çš„è·¯å¾„')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯')

    args = parser.parse_args()

    if not Path(args.csv_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        return 1

    # åˆ›å»ºè§£æå™¨
    parser = PPODataParser(args.csv_file)

    # åŠ è½½æ•°æ®
    if not parser.load_data():
        print("âŒ æ— æ³•åŠ è½½PPOæ•°æ®")
        return 1

    # æ˜¾ç¤ºæ‘˜è¦
    if not args.quiet:
        parser.display_summary()

    # å¯¼å‡ºæ•°æ®
    if args.export:
        success = parser.export_clean_data(args.export)
        if not success:
            return 1

    return 0


if __name__ == "__main__":
    exit(main())