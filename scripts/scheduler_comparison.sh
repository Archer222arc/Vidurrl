#!/bin/bash

# =============================================================================
# è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”è¯„æµ‹è„šæœ¬
#
# å¯¹æ¯”PPOã€Randomã€Round Robinã€LORè°ƒåº¦å™¨åœ¨ç›¸åŒé…ç½®ä¸‹çš„æ€§èƒ½
# è¾“å‡ºå¹³å‡å»¶è¿Ÿå’Œååé‡æŒ‡æ ‡ï¼Œä¸¥æ ¼éµå¾ªCLAUDE.mdé¡¹ç›®è§„èŒƒ
# =============================================================================

set -e

REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
export PYTHONPATH="${REPO_ROOT}"

# é…ç½®å‚æ•°
COMPARISON_ID=$(date +%Y%m%d_%H%M%S)
NUM_REQUESTS=500
QPS=3
NUM_REPLICAS=4
OUTPUT_DIR="./outputs/runs/scheduler_comparison/run_${COMPARISON_ID}"
RESULTS_DIR="${OUTPUT_DIR}/results"
CHECKPOINT_PATH="/Users/ruicheng/Documents/GitHub/Vidur/Vidur_arc2/outputs/checkpoints/latest.pt"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "${OUTPUT_DIR}"
mkdir -p "${RESULTS_DIR}"

echo "ğŸš€ å¼€å§‹è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”è¯„æµ‹ - Run ID: ${COMPARISON_ID}"
echo "ğŸ“‹ æµ‹è¯•é…ç½®:"
echo "   - è¯·æ±‚æ•°é‡: ${NUM_REQUESTS}"
echo "   - QPS: ${QPS}"
echo "   - å‰¯æœ¬æ•°: ${NUM_REPLICAS}"
echo "   - è¾“å‡ºç›®å½•: ${OUTPUT_DIR}"
echo ""

# åŸºç¡€å‘½ä»¤å‚æ•°
BASE_PARAMS="
  --cluster_config_num_replicas ${NUM_REPLICAS}
  --synthetic_request_generator_config_num_requests ${NUM_REQUESTS}
  --interval_generator_config_type poisson
  --poisson_request_interval_generator_config_qps ${QPS}
  --metrics_config_subsamples 200000
"

# =============================================================================
# æµ‹è¯•1: PPOè°ƒåº¦å™¨ (ä½¿ç”¨checkpoint)
# =============================================================================
echo "ğŸ“Š [1/4] æµ‹è¯• PPO è°ƒåº¦å™¨..."

python -m vidur.main \
  --global_scheduler_config_type ppo_modular \
  ${BASE_PARAMS} \
  --p_p_o_global_scheduler_modular_config_debug_dump_global_state \
  --p_p_o_global_scheduler_modular_config_max_queue_requests_per_replica 8 \
  --p_p_o_global_scheduler_modular_config_inference_only \
  --p_p_o_global_scheduler_modular_config_load_checkpoint "${CHECKPOINT_PATH}" \
  --no-p_p_o_global_scheduler_modular_config_enable_tensorboard \
  --no-p_p_o_global_scheduler_modular_config_enable_checkpoints \
  2>&1 | tee "${RESULTS_DIR}/ppo_output.log"

if [ $? -eq 0 ]; then
    echo "âœ… PPO è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ"
else
    echo "âŒ PPO è°ƒåº¦å™¨æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: ${RESULTS_DIR}/ppo_output.log"
fi

# =============================================================================
# æµ‹è¯•2: Randomè°ƒåº¦å™¨
# =============================================================================
echo "ğŸ“Š [2/4] æµ‹è¯• Random è°ƒåº¦å™¨..."

python -m vidur.main \
  --global_scheduler_config_type random \
  ${BASE_PARAMS} \
  2>&1 | tee "${RESULTS_DIR}/random_output.log"

if [ $? -eq 0 ]; then
    echo "âœ… Random è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ"
else
    echo "âŒ Random è°ƒåº¦å™¨æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: ${RESULTS_DIR}/random_output.log"
fi

# =============================================================================
# æµ‹è¯•3: Round Robinè°ƒåº¦å™¨
# =============================================================================
echo "ğŸ“Š [3/4] æµ‹è¯• Round Robin è°ƒåº¦å™¨..."

python -m vidur.main \
  --global_scheduler_config_type round_robin \
  ${BASE_PARAMS} \
  2>&1 | tee "${RESULTS_DIR}/round_robin_output.log"

if [ $? -eq 0 ]; then
    echo "âœ… Round Robin è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ"
else
    echo "âŒ Round Robin è°ƒåº¦å™¨æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: ${RESULTS_DIR}/round_robin_output.log"
fi

# =============================================================================
# æµ‹è¯•4: LORè°ƒåº¦å™¨
# =============================================================================
echo "ğŸ“Š [4/4] æµ‹è¯• LOR è°ƒåº¦å™¨..."

python -m vidur.main \
  --global_scheduler_config_type lor \
  ${BASE_PARAMS} \
  2>&1 | tee "${RESULTS_DIR}/lor_output.log"

if [ $? -eq 0 ]; then
    echo "âœ… LOR è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ"
else
    echo "âŒ LOR è°ƒåº¦å™¨æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: ${RESULTS_DIR}/lor_output.log"
fi

# =============================================================================
# ç»“æœåˆ†æå’Œæ±‡æ€»
# =============================================================================
echo ""
echo "ğŸ” åˆ†ææµ‹è¯•ç»“æœ..."

# åˆ›å»ºç»“æœåˆ†æè„šæœ¬
cat > "${OUTPUT_DIR}/analyze_results.py" << 'EOF'
#!/usr/bin/env python3
"""
è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”ç»“æœåˆ†æè„šæœ¬

ä»å„è°ƒåº¦å™¨çš„è¾“å‡ºæ—¥å¿—ä¸­æå–æ€§èƒ½æŒ‡æ ‡ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import re
import sys
import json
from pathlib import Path
from typing import Dict, Optional, Any

def extract_metrics_from_log(log_path: Path) -> Dict[str, Any]:
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–æ€§èƒ½æŒ‡æ ‡"""
    if not log_path.exists():
        return {"error": f"Log file not found: {log_path}"}

    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"Failed to read log: {e}"}

    metrics = {}

    # æå–å¹³å‡å»¶è¿Ÿ
    latency_patterns = [
        r'Average latency:\s*([\d.]+)',
        r'average_latency[:\s]+([\d.]+)',
        r'latency.*?(\d+\.\d+)',
        r'å»¶è¿Ÿ.*?(\d+\.\d+)',
    ]

    for pattern in latency_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metrics['average_latency'] = float(match.group(1))
            break

    # æå–ååé‡
    throughput_patterns = [
        r'Throughput:\s*([\d.]+)',
        r'throughput[:\s]+([\d.]+)',
        r'requests/second.*?(\d+\.\d+)',
        r'ååé‡.*?(\d+\.\d+)',
    ]

    for pattern in throughput_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metrics['throughput'] = float(match.group(1))
            break

    # æå–æ€»å¤„ç†æ—¶é—´
    duration_patterns = [
        r'Total simulation time:\s*([\d.]+)',
        r'execution time.*?(\d+\.\d+)',
        r'duration.*?(\d+\.\d+)',
    ]

    for pattern in duration_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metrics['total_duration'] = float(match.group(1))
            break

    # å¦‚æœæ‰¾ä¸åˆ°æŒ‡æ ‡ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹æå–
    if not metrics:
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
        error_patterns = [
            r'Error.*',
            r'Exception.*',
            r'Traceback.*',
            r'Failed.*'
        ]

        for pattern in error_patterns:
            match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)
            if match:
                metrics['error'] = match.group(0)[:200]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
                break

    return metrics

def generate_comparison_report(results_dir: Path) -> str:
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""

    schedulers = {
        'PPO': 'ppo_output.log',
        'Random': 'random_output.log',
        'Round Robin': 'round_robin_output.log',
        'LOR': 'lor_output.log'
    }

    all_metrics = {}

    # æå–å„è°ƒåº¦å™¨æŒ‡æ ‡
    for scheduler_name, log_file in schedulers.items():
        log_path = results_dir / log_file
        metrics = extract_metrics_from_log(log_path)
        all_metrics[scheduler_name] = metrics

    # ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append("=" * 80)
    report.append("ğŸ“Š è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
    report.append("=" * 80)
    report.append("")

    # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”è¡¨
    report.append("## æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
    report.append("")
    report.append(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡å»¶è¿Ÿ(s)':<12} {'ååé‡(req/s)':<15} {'çŠ¶æ€':<10}")
    report.append("-" * 60)

    valid_results = {}

    for scheduler_name in ['PPO', 'Random', 'Round Robin', 'LOR']:
        metrics = all_metrics[scheduler_name]

        if 'error' in metrics:
            status = "âŒ å¤±è´¥"
            latency_str = "N/A"
            throughput_str = "N/A"
        else:
            latency = metrics.get('average_latency', 'N/A')
            throughput = metrics.get('throughput', 'N/A')

            if latency != 'N/A' and throughput != 'N/A':
                status = "âœ… æˆåŠŸ"
                latency_str = f"{latency:.4f}"
                throughput_str = f"{throughput:.4f}"
                valid_results[scheduler_name] = metrics
            else:
                status = "âš ï¸ éƒ¨åˆ†"
                latency_str = f"{latency:.4f}" if latency != 'N/A' else "N/A"
                throughput_str = f"{throughput:.4f}" if throughput != 'N/A' else "N/A"

        report.append(f"{scheduler_name:<15} {latency_str:<12} {throughput_str:<15} {status:<10}")

    report.append("")

    # æ€§èƒ½æ’å
    if valid_results:
        report.append("## æ€§èƒ½æ’å")
        report.append("")

        # å»¶è¿Ÿæ’å (è¶Šä½è¶Šå¥½)
        if any('average_latency' in metrics for metrics in valid_results.values()):
            latency_ranking = sorted(
                [(name, metrics.get('average_latency', float('inf')))
                 for name, metrics in valid_results.items()
                 if 'average_latency' in metrics],
                key=lambda x: x[1]
            )

            report.append("### å¹³å‡å»¶è¿Ÿæ’å (è¶Šä½è¶Šå¥½):")
            for i, (name, latency) in enumerate(latency_ranking, 1):
                report.append(f"{i}. {name}: {latency:.4f}s")
            report.append("")

        # ååé‡æ’å (è¶Šé«˜è¶Šå¥½)
        if any('throughput' in metrics for metrics in valid_results.values()):
            throughput_ranking = sorted(
                [(name, metrics.get('throughput', 0))
                 for name, metrics in valid_results.items()
                 if 'throughput' in metrics],
                key=lambda x: x[1], reverse=True
            )

            report.append("### ååé‡æ’å (è¶Šé«˜è¶Šå¥½):")
            for i, (name, throughput) in enumerate(throughput_ranking, 1):
                report.append(f"{i}. {name}: {throughput:.4f} req/s")
            report.append("")

    # è¯¦ç»†ç»“æœ
    report.append("## è¯¦ç»†æµ‹è¯•ç»“æœ")
    report.append("")

    for scheduler_name, metrics in all_metrics.items():
        report.append(f"### {scheduler_name} è°ƒåº¦å™¨")
        if 'error' in metrics:
            report.append(f"âŒ æµ‹è¯•å¤±è´¥: {metrics['error']}")
        else:
            for key, value in metrics.items():
                if key != 'error':
                    report.append(f"- {key}: {value}")
        report.append("")

    # æµ‹è¯•ç¯å¢ƒä¿¡æ¯
    report.append("## æµ‹è¯•é…ç½®")
    report.append(f"- è¯·æ±‚æ•°é‡: {NUM_REQUESTS}")
    report.append(f"- QPS: {QPS}")
    report.append(f"- å‰¯æœ¬æ•°: {NUM_REPLICAS}")
    report.append(f"- æµ‹è¯•æ—¶é—´: {COMPARISON_ID}")
    report.append("")

    report.append("=" * 80)

    return "\n".join(report)

if __name__ == "__main__":
    results_dir = Path(sys.argv[1]) if len(sys.argv) > 1 else Path(".")

    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    import os
    global NUM_REQUESTS, QPS, NUM_REPLICAS, COMPARISON_ID
    NUM_REQUESTS = os.environ.get('NUM_REQUESTS', '500')
    QPS = os.environ.get('QPS', '2')
    NUM_REPLICAS = os.environ.get('NUM_REPLICAS', '4')
    COMPARISON_ID = os.environ.get('COMPARISON_ID', 'unknown')

    report = generate_comparison_report(results_dir)
    print(report)

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = results_dir.parent / f"scheduler_comparison_report_{COMPARISON_ID}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
EOF

# è®¾ç½®ç¯å¢ƒå˜é‡å¹¶è¿è¡Œåˆ†æè„šæœ¬
export NUM_REQUESTS="${NUM_REQUESTS}"
export QPS="${QPS}"
export NUM_REPLICAS="${NUM_REPLICAS}"
export COMPARISON_ID="${COMPARISON_ID}"

python3 "${OUTPUT_DIR}/analyze_results.py" "${RESULTS_DIR}"

# =============================================================================
# å¿«é€Ÿæ€»è§ˆ - ç›´æ¥è¾“å‡ºç®€æ´çš„æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
# =============================================================================
echo ""
echo "ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»è§ˆ"
echo "=================================================================="

# åˆ›å»ºå¢å¼ºçš„æ€»è§ˆè„šæœ¬ - ä»simulator_output CSVæ–‡ä»¶æå–çœŸå®æŒ‡æ ‡
cat > "${OUTPUT_DIR}/quick_summary.py" << 'EOF'
#!/usr/bin/env python3
import re
import sys
import csv
import pandas as pd
from pathlib import Path

def find_latest_simulator_output(scheduler_name):
    """æŸ¥æ‰¾å¯¹åº”è°ƒåº¦å™¨çš„æœ€æ–°simulatorè¾“å‡ºç›®å½•"""
    simulator_output_dir = Path("outputs/simulator_output")
    if not simulator_output_dir.exists():
        return None

    # è·å–æ‰€æœ‰è¾“å‡ºç›®å½•ï¼ŒæŒ‰æ—¶é—´æ’åº
    output_dirs = sorted([d for d in simulator_output_dir.iterdir() if d.is_dir()],
                        key=lambda x: x.name, reverse=True)

    # æŸ¥æ‰¾æœ€è¿‘4ä¸ªç›®å½•ï¼ˆå¯¹åº”4ä¸ªè°ƒåº¦å™¨çš„æµ‹è¯•ï¼‰
    return output_dirs[:4] if len(output_dirs) >= 4 else output_dirs

def extract_metrics_from_csv(simulator_dir):
    """ä»CSVæ–‡ä»¶ä¸­æå–çœŸå®çš„æ€§èƒ½æŒ‡æ ‡"""
    if not simulator_dir or not simulator_dir.exists():
        return {"status": "âŒ æœªæ‰¾åˆ°", "latency": "N/A", "throughput": "N/A"}

    try:
        # è¯»å–throughput_latency.csvè·å–å¹³å‡æŒ‡æ ‡
        throughput_file = simulator_dir / "throughput_latency.csv"
        request_metrics_file = simulator_dir / "request_metrics.csv"

        if throughput_file.exists():
            df = pd.read_csv(throughput_file)
            # è®¡ç®—æœ€åé˜¶æ®µçš„å¹³å‡ååé‡å’Œå»¶è¿Ÿï¼ˆæ’é™¤å¯åŠ¨é˜¶æ®µçš„0å€¼ï¼‰
            valid_data = df[(df['throughput'] > 0) & (df['avg_latency'] > 0)]

            if len(valid_data) > 0:
                avg_throughput = valid_data['throughput'].mean()
                avg_latency = valid_data['avg_latency'].mean()
                return {
                    "status": "âœ… æˆåŠŸ",
                    "latency": f"{avg_latency:.3f}",
                    "throughput": f"{avg_throughput:.3f}"
                }

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»request_metrics.csvè®¡ç®—
        if request_metrics_file.exists():
            df = pd.read_csv(request_metrics_file)
            if len(df) > 0:
                avg_latency = df['request_e2e_time'].mean()
                # ä¼°ç®—ååé‡ï¼ˆè¯·æ±‚æ•°/æ€»æ—¶é—´ï¼‰
                total_time = df['request_e2e_time'].sum() / len(df) if len(df) > 0 else 1
                throughput = len(df) / max(total_time, 1)
                return {
                    "status": "âœ… æˆåŠŸ",
                    "latency": f"{avg_latency:.3f}",
                    "throughput": f"{throughput:.3f}"
                }

        return {"status": "âš ï¸ æ— æ•°æ®", "latency": "N/A", "throughput": "N/A"}

    except Exception as e:
        return {"status": "âŒ è§£æå¤±è´¥", "latency": "N/A", "throughput": "N/A"}

def extract_key_metrics_fallback(log_path):
    """ä»æ—¥å¿—æ–‡ä»¶æå–æŒ‡æ ‡çš„å¤‡ç”¨æ–¹æ¡ˆ"""
    if not log_path.exists():
        return {"status": "âŒ æœªæ‰¾åˆ°", "latency": "N/A", "throughput": "N/A"}

    try:
        with open(log_path, 'r') as f:
            content = f.read()

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "Traceback" in content or "Error" in content:
            return {"status": "âŒ é”™è¯¯", "latency": "N/A", "throughput": "N/A"}

        return {"status": "âš ï¸ è¿è¡Œå®Œæˆ", "latency": "N/A", "throughput": "N/A"}

    except Exception as e:
        return {"status": "âŒ è§£æå¤±è´¥", "latency": "N/A", "throughput": "N/A"}

def main():
    results_dir = Path(sys.argv[1]) if len(sys.argv) > 1 else Path(".")

    schedulers = [
        ("PPO", "ppo_output.log"),
        ("Random", "random_output.log"),
        ("RoundRobin", "round_robin_output.log"),
        ("LOR", "lor_output.log")
    ]

    print(f"{'è°ƒåº¦å™¨':<12} {'çŠ¶æ€':<10} {'å»¶è¿Ÿ(s)':<10} {'ååé‡(req/s)':<12}")
    print("-" * 52)

    # è·å–æœ€æ–°çš„simulatorè¾“å‡ºç›®å½•
    simulator_dirs = find_latest_simulator_output("all")

    for i, (name, log_file) in enumerate(schedulers):
        # å°è¯•ä»CSVæ–‡ä»¶æå–æŒ‡æ ‡
        if simulator_dirs and i < len(simulator_dirs):
            # åå‘æ˜ å°„ï¼šç¬¬iä¸ªè°ƒåº¦å™¨å¯¹åº”å€’æ•°ç¬¬(i+1)ä¸ªç›®å½•
            # PPO(i=0) -> simulator_dirs[3], Random(i=1) -> simulator_dirs[2], etc.
            reverse_index = len(simulator_dirs) - 1 - i
            metrics = extract_metrics_from_csv(simulator_dirs[reverse_index])
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ—¥å¿—æ–‡ä»¶æå–
            log_path = results_dir / log_file
            metrics = extract_key_metrics_fallback(log_path)

        print(f"{name:<12} {metrics['status']:<10} {metrics['latency']:<10} {metrics['throughput']:<12}")

if __name__ == "__main__":
    main()
EOF

# è¿è¡Œå¿«é€Ÿæ€»è§ˆ
python3 "${OUTPUT_DIR}/quick_summary.py" "${RESULTS_DIR}"

echo ""
echo "ğŸ‰ è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”è¯„æµ‹å®Œæˆï¼"
echo "ğŸ“‚ ç»“æœç›®å½•: ${OUTPUT_DIR}"
echo "ğŸ“Š è¯¦ç»†æ—¥å¿—: ${RESULTS_DIR}/"
echo "ğŸ“ å¯¹æ¯”æŠ¥å‘Š: ${OUTPUT_DIR}/scheduler_comparison_report_${COMPARISON_ID}.md"
echo ""
echo "ğŸ”— å¿«é€ŸæŸ¥çœ‹ç»“æœ:"
echo "   cat ${OUTPUT_DIR}/scheduler_comparison_report_${COMPARISON_ID}.md"
