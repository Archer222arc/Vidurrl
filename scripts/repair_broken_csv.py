#!/usr/bin/env python3
"""
ä¿®å¤æŸåçš„PPOæŒ‡æ ‡CSVæ–‡ä»¶

è¯¥å·¥å…·å¯ä»¥ä¿®å¤ç”±äºCSVæ¶æ„æ¼”åŒ–é—®é¢˜å¯¼è‡´çš„"æŸå"CSVæ–‡ä»¶ï¼Œ
æ¢å¤ç¼ºå¤±çš„PPOè®­ç»ƒæŒ‡æ ‡åˆ—åã€‚
"""

import csv
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Any


def analyze_csv_structure(csv_path: str) -> Dict[str, Any]:
    """
    åˆ†æCSVæ–‡ä»¶ç»“æ„

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    print(f"ğŸ” åˆ†æCSVæ–‡ä»¶: {csv_path}")

    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            rows = list(reader)

        print(f"ğŸ“Š åˆ—æ•°: {len(header)}")
        print(f"ğŸ“ æ•°æ®è¡Œæ•°: {len(rows)}")

        # æ£€æŸ¥æ•°æ®ç±»å‹åˆ†å¸ƒ
        data_types = {}
        ppo_rows = []

        for i, row in enumerate(rows):
            if len(row) > len(header):
                # æ‰¾åˆ°è¶…å‡ºå¤´éƒ¨é•¿åº¦çš„è¡Œ
                try:
                    data_type_idx = header.index('data_type') if 'data_type' in header else None
                    if data_type_idx is not None and len(row) > data_type_idx:
                        data_type = row[data_type_idx]
                        data_types[data_type] = data_types.get(data_type, 0) + 1

                        if data_type == 'ppo_update':
                            ppo_rows.append((i + 1, row))  # +1 for 1-based line numbering

                except (ValueError, IndexError):
                    pass

        return {
            'header': header,
            'rows': rows,
            'data_types': data_types,
            'ppo_rows': ppo_rows,
            'has_overflow': any(len(row) > len(header) for row in rows)
        }

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None


def detect_missing_ppo_columns(analysis: Dict[str, Any]) -> List[str]:
    """
    æ£€æµ‹ç¼ºå¤±çš„PPOåˆ—

    Args:
        analysis: CSVåˆ†æç»“æœ

    Returns:
        ç¼ºå¤±çš„PPOåˆ—ååˆ—è¡¨
    """
    header = analysis['header']
    ppo_rows = analysis['ppo_rows']

    # å·²çŸ¥çš„PPOæŒ‡æ ‡åˆ—
    known_ppo_columns = [
        'pi_loss', 'vf_loss', 'entropy', 'approx_kl',
        'clipfrac', 'pg_grad_norm', 'explained_var', 'lr',
        'rollout_length', 'buffer_size'
    ]

    missing_columns = []
    for col in known_ppo_columns:
        if col not in header:
            missing_columns.append(col)

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºå¤´éƒ¨çš„æ•°æ®ï¼ˆè¯´æ˜ç¡®å®æœ‰ç¼ºå¤±åˆ—ï¼‰
    if ppo_rows and missing_columns:
        # ä¼°ç®—å®é™…ç¼ºå¤±çš„åˆ—æ•°
        max_overflow = max(len(row) - len(header) for _, row in ppo_rows)
        print(f"ğŸ” æ£€æµ‹åˆ° {max_overflow} ä¸ªè¶…å‡ºåˆ—ï¼Œé¢„æœŸç¼ºå¤±åˆ—: {len(missing_columns)}")

    return missing_columns


def repair_csv_file(csv_path: str, output_path: str = None) -> bool:
    """
    ä¿®å¤CSVæ–‡ä»¶

    Args:
        csv_path: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰

    Returns:
        ä¿®å¤æ˜¯å¦æˆåŠŸ
    """
    # åˆ†ææ–‡ä»¶
    analysis = analyze_csv_structure(csv_path)
    if not analysis:
        return False

    # æ£€æµ‹ç¼ºå¤±åˆ—
    missing_columns = detect_missing_ppo_columns(analysis)
    if not missing_columns:
        print("âœ… æ–‡ä»¶çœ‹èµ·æ¥æ²¡æœ‰é—®é¢˜ï¼Œæ— éœ€ä¿®å¤")
        return True

    print(f"ğŸ”§ æ£€æµ‹åˆ°ç¼ºå¤±çš„PPOåˆ—: {missing_columns}")

    # åˆ›å»ºæ–°çš„å¤´éƒ¨
    original_header = analysis['header']
    new_header = original_header + missing_columns

    print(f"ğŸ“Š åŸå§‹åˆ—æ•°: {len(original_header)}")
    print(f"ğŸ“ˆ ä¿®å¤ååˆ—æ•°: {len(new_header)}")

    # å‡†å¤‡è¾“å‡ºè·¯å¾„
    if output_path is None:
        output_path = csv_path
        backup_path = csv_path + '.backup'
        # åˆ›å»ºå¤‡ä»½
        try:
            Path(csv_path).rename(backup_path)
            print(f"ğŸ’¾ å·²åˆ›å»ºå¤‡ä»½: {backup_path}")
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return False
    else:
        backup_path = None

    try:
        # é‡å†™æ–‡ä»¶
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # å†™å…¥æ–°å¤´éƒ¨
            writer.writerow(new_header)

            # å†™å…¥æ•°æ®è¡Œ
            for row in analysis['rows']:
                # ç¡®ä¿æ¯è¡Œéƒ½æœ‰è¶³å¤Ÿçš„åˆ—
                padded_row = row + [''] * (len(new_header) - len(row))
                writer.writerow(padded_row)

        print(f"âœ… ä¿®å¤å®Œæˆï¼Œè¾“å‡ºåˆ°: {output_path}")

        # éªŒè¯ä¿®å¤ç»“æœ
        if verify_repair(output_path, missing_columns):
            print("ğŸ‰ ä¿®å¤éªŒè¯æˆåŠŸ!")
            # åˆ é™¤å¤‡ä»½ï¼ˆå¦‚æœåˆ›å»ºäº†ï¼‰
            if backup_path and Path(backup_path).exists():
                Path(backup_path).unlink()
                print("ğŸ—‘ï¸  å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶")
            return True
        else:
            print("âŒ ä¿®å¤éªŒè¯å¤±è´¥")
            # æ¢å¤å¤‡ä»½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if backup_path and Path(backup_path).exists():
                Path(backup_path).rename(output_path)
                print("ğŸ”„ å·²æ¢å¤å¤‡ä»½æ–‡ä»¶")
            return False

    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        # æ¢å¤å¤‡ä»½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if backup_path and Path(backup_path).exists():
            Path(backup_path).rename(output_path)
            print("ğŸ”„ å·²æ¢å¤å¤‡ä»½æ–‡ä»¶")
        return False


def verify_repair(csv_path: str, expected_columns: List[str]) -> bool:
    """
    éªŒè¯ä¿®å¤ç»“æœ

    Args:
        csv_path: ä¿®å¤åçš„CSVæ–‡ä»¶è·¯å¾„
        expected_columns: æœŸæœ›çš„åˆ—å

    Returns:
        éªŒè¯æ˜¯å¦æˆåŠŸ
    """
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)

        missing = [col for col in expected_columns if col not in header]
        if missing:
            print(f"âš ï¸  éªŒè¯å¤±è´¥ï¼Œä»ç¼ºå¤±åˆ—: {missing}")
            return False

        print("âœ… éªŒè¯æˆåŠŸï¼Œæ‰€æœ‰é¢„æœŸåˆ—éƒ½å­˜åœ¨")
        return True

    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description='ä¿®å¤æŸåçš„PPOæŒ‡æ ‡CSVæ–‡ä»¶')
    parser.add_argument('csv_file', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è¦†ç›–åŸæ–‡ä»¶ï¼‰')
    parser.add_argument('--analyze-only', action='store_true', help='ä»…åˆ†æï¼Œä¸ä¿®å¤')

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.csv_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        return 1

    if args.analyze_only:
        # ä»…åˆ†ææ¨¡å¼
        analysis = analyze_csv_structure(args.csv_file)
        if analysis:
            missing = detect_missing_ppo_columns(analysis)
            print(f"\nğŸ“Š åˆ†æç»“æœ:")
            print(f"  æ•°æ®ç±»å‹åˆ†å¸ƒ: {analysis['data_types']}")
            print(f"  æ˜¯å¦æœ‰æ•°æ®æº¢å‡º: {'æ˜¯' if analysis['has_overflow'] else 'å¦'}")
            print(f"  ç¼ºå¤±çš„PPOåˆ—: {missing if missing else 'æ— '}")
            print(f"  PPOæ›´æ–°è¡Œæ•°: {len(analysis['ppo_rows'])}")
        return 0

    # ä¿®å¤æ¨¡å¼
    print("ğŸ”§ å¼€å§‹ä¿®å¤CSVæ–‡ä»¶...")
    success = repair_csv_file(args.csv_file, args.output)

    if success:
        print("\nğŸ‰ ä¿®å¤æˆåŠŸå®Œæˆ!")
        if args.output:
            print(f"ğŸ“ ä¿®å¤åçš„æ–‡ä»¶: {args.output}")
        else:
            print(f"ğŸ“ åŸæ–‡ä»¶å·²æ›´æ–°: {args.csv_file}")
        return 0
    else:
        print("\nğŸ’¥ ä¿®å¤å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())