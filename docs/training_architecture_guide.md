# Viduræ™ºèƒ½è°ƒåº¦å™¨ - è®­ç»ƒæ¶æ„å®Œæ•´æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†Viduræ™ºèƒ½è°ƒåº¦å™¨é¡¹ç›®ä¸­çš„è®­ç»ƒç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»„ä»¶æ¥å£ã€çŠ¶æ€å®šä¹‰ã€åŠ¨ä½œç©ºé—´å’Œè®­ç»ƒæµç¨‹ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¼ºåŒ–å­¦ä¹ (PPO)æ–¹æ³•æ¥ä¼˜åŒ–LLMæ¨ç†æœåŠ¡çš„è¯·æ±‚è°ƒåº¦ç­–ç•¥ã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ç»„ä»¶

### 1. è®­ç»ƒç»„ä»¶å±‚æ¬¡ç»“æ„

```
src/core/
â”œâ”€â”€ models/                          # ç¥ç»ç½‘ç»œæ¨¡å‹
â”‚   â”œâ”€â”€ actor_critic.py             # Actor-Criticç½‘ç»œæ¶æ„
â”‚   â””â”€â”€ state_builder.py            # çŠ¶æ€æ„å»ºå™¨
â”œâ”€â”€ algorithms/                      # è®­ç»ƒç®—æ³•
â”‚   â”œâ”€â”€ ppo_trainer.py              # PPOè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ rollout_buffer.py           # ç»éªŒå›æ”¾ç¼“å†²åŒº
â”‚   â””â”€â”€ rewards/
â”‚       â””â”€â”€ reward_calculator.py    # å¥–åŠ±è®¡ç®—å™¨
â””â”€â”€ utils/                          # å·¥å…·ç»„ä»¶
    â”œâ”€â”€ normalizers.py              # çŠ¶æ€æ ‡å‡†åŒ–
    â”œâ”€â”€ temperature_controller.py   # æ¸©åº¦æ§åˆ¶
    â””â”€â”€ monitoring/                 # ç›‘æ§ç³»ç»Ÿ
        â”œâ”€â”€ tensorboard_monitor.py  # TensorBoardé›†æˆ
        â””â”€â”€ metrics_exporter.py     # æŒ‡æ ‡å¯¼å‡º
```

### 2. è°ƒåº¦å™¨é›†æˆç‚¹

```
vidur/scheduler/global_scheduler/
â””â”€â”€ ppo_scheduler_modular.py        # PPOè°ƒåº¦å™¨ä¸»å…¥å£
```

## ğŸ§  ç¥ç»ç½‘ç»œæ¶æ„

### ActorCritic æ¨¡å‹ (`src/core/models/actor_critic.py`)

**åŠŸèƒ½**: å®ç°å…±äº«ç‰¹å¾æå–å’Œåˆ†ç¦»çš„Actor-Criticç½‘ç»œæ¶æ„

#### ç½‘ç»œç»„ä»¶
```python
# ç‰¹å¾æŠ•å½±å±‚ (å¤šç»´çŠ¶æ€æ ‡å‡†åŒ–)
feature_proj: nn.Sequential([
    nn.LayerNorm(state_dim),           # è¾“å…¥æ ‡å‡†åŒ–
    nn.Linear(state_dim, proj_dim),    # æŠ•å½±åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´
    nn.GELU(),                         # å¹³æ»‘æ¿€æ´»å‡½æ•°
    nn.Linear(proj_dim, hidden_size)   # é™ç»´åˆ°éšè—ç»´åº¦
])

# å…±äº«MLPç¼–ç å™¨ (å¯é…ç½®å±‚æ•°)
shared_mlp: nn.Sequential([
    nn.Linear(hidden_size, hidden_size),
    nn.LayerNorm(hidden_size),
    nn.ReLU()
]) * (1 + layer_N)

# GRUé€’å½’å±‚ (æ—¶åºå»ºæ¨¡)
gru: nn.GRU(
    input_size=hidden_size,
    hidden_size=hidden_size,
    num_layers=gru_layers,
    batch_first=False  # (T,N,H)æ ¼å¼
)

# è§£è€¦çš„Actoråˆ†æ”¯ (ç­–ç•¥ç½‘ç»œ)
actor_branch: nn.Sequential([
    nn.Linear(hidden_size, hidden_size) * 2,  # 2å±‚å…¨è¿æ¥
    nn.LayerNorm + nn.ReLU,                   # æ ‡å‡†åŒ–å’Œæ¿€æ´»
    nn.Linear(hidden_size, action_dim),        # è¾“å‡ºåŠ¨ä½œlogits
    nn.LayerNorm + nn.Tanh                    # æœ‰ç•Œè¾“å‡º
])

# è§£è€¦çš„Criticåˆ†æ”¯ (ä»·å€¼ç½‘ç»œ)
critic_branch: nn.Sequential([
    nn.Linear(hidden_size, hidden_size) * 2,  # 2å±‚å…¨è¿æ¥
    nn.LayerNorm + nn.ReLU,                   # æ ‡å‡†åŒ–å’Œæ¿€æ´»
    nn.GRU(separate),                         # ç‹¬ç«‹GRU
    nn.Linear(hidden_size, 1)                 # çŠ¶æ€ä»·å€¼è¾“å‡º
])
```

#### å…³é”®æ¥å£æ–¹æ³•

**1. åŠ¨ä½œé‡‡æ ·æ¥å£** (`act_value`)
```python
def act_value(
    s: torch.Tensor,        # çŠ¶æ€ (N, state_dim)
    hxs: torch.Tensor,      # éšè—çŠ¶æ€ (layers, N, hidden_size)
    masks: torch.Tensor,    # é‡ç½®æ©ç  (N, 1)
    temperature: float = 1.0 # æ¸©åº¦å‚æ•° (æ¢ç´¢æ§åˆ¶)
) -> Tuple[action, log_prob, value, new_hxs]
```

**2. åŠ¨ä½œè¯„ä¼°æ¥å£** (`evaluate_actions`)
```python
def evaluate_actions(
    s: torch.Tensor,        # çŠ¶æ€æ‰¹æ¬¡
    hxs: torch.Tensor,      # éšè—çŠ¶æ€
    masks: torch.Tensor,    # é‡ç½®æ©ç 
    a: torch.Tensor         # å¾…è¯„ä¼°åŠ¨ä½œ
) -> Tuple[log_prob, entropy, value, new_hxs]
```

### StateBuilder çŠ¶æ€æ„å»ºå™¨ (`src/core/models/state_builder.py`)

**åŠŸèƒ½**: å°†å¤æ‚çš„è°ƒåº¦å™¨çŠ¶æ€è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯å¤„ç†çš„ç‰¹å¾å‘é‡

#### çŠ¶æ€æ„å»ºæµç¨‹

**1. å•å‰¯æœ¬çŠ¶æ€ç‰¹å¾** (`build_replica_state`)
```python
# åŸºç¡€èµ„æºç‰¹å¾ (11ç»´)
base_features = [
    queue_length,          # é˜Ÿåˆ—é•¿åº¦
    allocated_blocks,      # å·²åˆ†é…å†…å­˜å—
    total_blocks,         # æ€»å†…å­˜å—æ•°
    utilization_fraction, # åˆ©ç”¨ç‡
    available_fraction,   # å¯ç”¨ç‡
    running_batches,      # è¿è¡Œä¸­æ‰¹æ¬¡æ•°
    preempted_requests,   # è¢«æŠ¢å è¯·æ±‚æ•°
    allocation_map_size,  # åˆ†é…æ˜ å°„å¤§å°
    batch_capacity,       # æ‰¹æ¬¡å®¹é‡
    block_size,          # å†…å­˜å—å¤§å°
    num_stages           # æµæ°´çº¿é˜¶æ®µæ•°
]

# å¢å¼ºæ—¶åºç‰¹å¾ (8ç»´) - å¯é€‰
enhanced_features = [
    queue_delta,          # é˜Ÿåˆ—é•¿åº¦å˜åŒ–
    allocation_delta,     # åˆ†é…å˜åŒ–
    load_ema,            # è´Ÿè½½æŒ‡æ•°ç§»åŠ¨å¹³å‡
    queue_trend,         # é˜Ÿåˆ—è¶‹åŠ¿(æ–œç‡)
    queue_variance,      # é˜Ÿåˆ—æ–¹å·®
    historical_peak,     # å†å²å³°å€¼
    historical_low,      # å†å²ä½ç‚¹
    time_since_peak     # è·ç¦»å³°å€¼æ—¶é—´
]

# è¯·æ±‚ç‰¹å¾ (Kä¸ªè¯·æ±‚ Ã— 7ç»´/è¯·æ±‚)
request_features = [
    age,                 # è¯·æ±‚å¹´é¾„
    num_prefill_tokens, # é¢„å¡«å……ä»¤ç‰Œæ•°
    num_processed_tokens, # å·²å¤„ç†ä»¤ç‰Œæ•°
    remaining_prefill,   # å‰©ä½™é¢„å¡«å……
    completed,          # å®Œæˆæ ‡å¿—
    num_decode_tokens,  # è§£ç ä»¤ç‰Œæ•°
    priority           # è¯·æ±‚ä¼˜å…ˆçº§
] * max_queue_requests
```

**2. å…¨å±€çŠ¶æ€æ„å»º** (`build_global_state`)
```python
# çŠ¶æ€ç»„æˆ = æ‰€æœ‰å‰¯æœ¬çŠ¶æ€ + å…¨å±€ç‰¹å¾ + å¢å¼ºå…¨å±€ç‰¹å¾
global_state = [
    *replica_states,      # æ¯ä¸ªå‰¯æœ¬çš„å®Œæ•´çŠ¶æ€
    global_queue_length,  # å…¨å±€é˜Ÿåˆ—é•¿åº¦
    system_throughput,    # ç³»ç»Ÿååé‡
    average_latency,      # å¹³å‡å»¶è¿Ÿ

    # å¢å¼ºå…¨å±€ç‰¹å¾ (7ç»´) - å¯é€‰
    qps_current,         # å½“å‰QPS
    qps_ema,            # QPSæŒ‡æ•°ç§»åŠ¨å¹³å‡
    qps_trend,          # QPSè¶‹åŠ¿
    qps_variance,       # QPSæ–¹å·®
    system_load_balance, # ç³»ç»Ÿè´Ÿè½½å‡è¡¡åº¦
    global_queue_delta,  # å…¨å±€é˜Ÿåˆ—å˜åŒ–
    completion_rate     # è¯·æ±‚å®Œæˆç‡
]
```

**3. çŠ¶æ€ç»´åº¦è®¡ç®—**
```python
def get_state_dimension(num_replicas: int) -> int:
    replica_base = 11                                    # åŸºç¡€ç‰¹å¾
    enhanced_replica = 8 if enable_enhanced else 0      # å¢å¼ºç‰¹å¾
    request_features = max_queue_requests * 7            # è¯·æ±‚ç‰¹å¾

    per_replica_dim = replica_base + enhanced_replica + request_features
    global_base = 3                                      # åŸºç¡€å…¨å±€ç‰¹å¾
    enhanced_global = 7 if enable_enhanced else 0       # å¢å¼ºå…¨å±€ç‰¹å¾

    return num_replicas * per_replica_dim + global_base + enhanced_global
```

## ğŸ¯ åŠ¨ä½œç©ºé—´å®šä¹‰

### åŠ¨ä½œè¡¨ç¤º
- **åŠ¨ä½œç±»å‹**: ç¦»æ•£åŠ¨ä½œç©ºé—´
- **åŠ¨ä½œç»´åº¦**: `len(replica_ids)` (ç­‰äºå‰¯æœ¬æ•°é‡)
- **åŠ¨ä½œå«ä¹‰**: é€‰æ‹©å“ªä¸ªå‰¯æœ¬æ¥å¤„ç†å½“å‰è¯·æ±‚
- **åŠ¨ä½œèŒƒå›´**: `[0, num_replicas-1]`

### åŠ¨ä½œé€‰æ‹©æµç¨‹
```python
# 1. çŠ¶æ€è¾“å…¥ -> Actorç½‘ç»œ -> åŠ¨ä½œlogits
logits = actor_network(state)  # (batch_size, num_replicas)

# 2. æ¸©åº¦ç¼©æ”¾ (æ¢ç´¢æ§åˆ¶)
scaled_logits = logits / temperature

# 3. æ¦‚ç‡åˆ†å¸ƒæ„é€ 
distribution = Categorical(logits=scaled_logits)

# 4. åŠ¨ä½œé‡‡æ ·
action = distribution.sample()  # èŒƒå›´: [0, num_replicas-1]

# 5. è¯·æ±‚åˆ†é…
selected_replica_id = replica_ids[action]
scheduler.assign_request(request, selected_replica_id)
```

### æ¸©åº¦æ§åˆ¶æœºåˆ¶
```python
# åŠ¨æ€æ¸©åº¦è®¡ç®— (TemperatureController)
temperature = base_temp * pressure_factor

pressure_factor = f(
    qps_pressure,      # QPSå‹åŠ›
    latency_pressure,  # å»¶è¿Ÿå‹åŠ›
    load_balance,      # è´Ÿè½½å‡è¡¡åº¦
    delta_signals      # å˜åŒ–ä¿¡å·
)

# æ¸©åº¦èŒƒå›´: [min_temp=0.8, max_temp=3.0]
# é«˜æ¸©åº¦ -> æ›´å¤šæ¢ç´¢
# ä½æ¸©åº¦ -> æ›´å¤šåˆ©ç”¨
```

## ğŸ† å¥–åŠ±ç³»ç»Ÿ

### RewardCalculator (`src/core/algorithms/rewards/reward_calculator.py`)

**åŠŸèƒ½**: æ ¹æ®ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡è®¡ç®—å¼ºåŒ–å­¦ä¹ å¥–åŠ±ä¿¡å·

#### ä¸‰ç§å¥–åŠ±æ¨¡å¼

**1. Deltaæ¨¡å¼** (`mode="delta"`)
```python
# åŸºäºæŒ‡æ ‡å˜åŒ–çš„å¥–åŠ±
reward = (
    + delta_throughput              # é¼“åŠ±ååé‡æå‡
    - latency_weight * delta_latency # æƒ©ç½šå»¶è¿Ÿå¢åŠ 
    - balance_penalty               # è´Ÿè½½ä¸å‡è¡¡æƒ©ç½š
    - threshold_penalty             # è½¯å»¶è¿Ÿé˜ˆå€¼æƒ©ç½š
)
```

**2. Instantæ¨¡å¼** (`mode="instant"`)
```python
# åŸºäºå½“å‰æŒ‡æ ‡å€¼çš„å¥–åŠ±
reward = (
    + throughput                    # å½“å‰ååé‡å¥–åŠ±
    - latency_weight * latency      # å½“å‰å»¶è¿Ÿæƒ©ç½š
    - balance_penalty               # è´Ÿè½½ä¸å‡è¡¡æƒ©ç½š
    - threshold_penalty             # è½¯å»¶è¿Ÿé˜ˆå€¼æƒ©ç½š
)
```

**3. Hybridæ¨¡å¼** (`mode="hybrid"`) - **æ¨è**
```python
# ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿ
absolute_score = (throughput/target) - alpha*(latency/threshold)
delta_score = beta*norm_delta_tp - gamma*norm_delta_lat
logistic_penalty = kappa/(1 + exp(-(lat-threshold)/sigma))

reward = (
    + absolute_weight * absolute_score    # ç»å¯¹æ€§èƒ½åˆ†æ•°
    + delta_weight * delta_score         # æ”¹è¿›ä¿¡å·
    - load_balance_penalty               # è´Ÿè½½å‡è¡¡æƒ©ç½š
    - logistic_penalty                   # å¹³æ»‘å»¶è¿Ÿæƒ©ç½š
)
```

#### å¥–åŠ±ç»„ä»¶è¯¦è§£

**è´Ÿè½½å‡è¡¡æƒ©ç½š**
```python
def calculate_load_balance_penalty(replica_ids, get_scheduler_fn):
    queue_lengths = [len(get_scheduler_fn(rid)._request_queue)
                    for rid in replica_ids]
    mean_queue = sum(queue_lengths) / len(queue_lengths)

    if mean_queue == 0:
        return 0.0  # å®Œç¾å‡è¡¡

    # å˜å¼‚ç³»æ•° = æ ‡å‡†å·®/å‡å€¼
    cv = std(queue_lengths) / mean_queue
    return cv  # è¶Šé«˜è¶Šä¸å‡è¡¡
```

**è½¯å»¶è¿Ÿé˜ˆå€¼æƒ©ç½š**
```python
def calculate_latency_threshold_penalty(latency):
    if latency <= threshold:
        return 0.0

    # æŒ‡æ•°è½¯æƒ©ç½š
    excess = latency - threshold
    penalty = scale * (1.0 - exp(-excess))
    return penalty
```

## ğŸ”„ PPOè®­ç»ƒç®—æ³•

### PPOTrainer (`src/core/algorithms/ppo_trainer.py`)

**åŠŸèƒ½**: å®ç°Proximal Policy Optimizationç®—æ³•

#### æ ¸å¿ƒè¶…å‚æ•°
```python
lr: float = 3e-4           # å­¦ä¹ ç‡
clip_ratio: float = 0.2    # PPOè£å‰ªæ¯”ä¾‹
entropy_coef: float = 0.01 # ç†µç³»æ•°(æ¢ç´¢)
value_coef: float = 0.5    # ä»·å€¼æŸå¤±æƒé‡
epochs: int = 4            # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
minibatch_size: int = 64   # å°æ‰¹æ¬¡å¤§å°
max_grad_norm: float = 0.5 # æ¢¯åº¦è£å‰ª
target_kl: float = 0.01    # ç›®æ ‡KLæ•£åº¦
```

#### æŸå¤±å‡½æ•°ç»„æˆ
```python
# 1. PPOç­–ç•¥æŸå¤± (å¸¦è£å‰ª)
ratio = exp(new_logp - old_logp)
surr1 = ratio * advantages
surr2 = clip(ratio, 1-clip_ratio, 1+clip_ratio) * advantages
policy_loss = -min(surr1, surr2).mean()

# 2. ä»·å€¼å‡½æ•°æŸå¤± (å¸¦è£å‰ª)
value_clipped = old_values + clip(new_values - old_values,
                                 -clip_ratio, clip_ratio)
value_loss = max((new_values - returns)^2,
                (value_clipped - returns)^2).mean()

# 3. ç†µå¥–åŠ± (é¼“åŠ±æ¢ç´¢)
entropy_bonus = entropy_coef * entropy.mean()

# 4. KLæ­£åˆ™åŒ– (é˜²æ­¢è¿‡å¤§æ›´æ–°)
kl_penalty = kl_coef * kl_divergence

# 5. æ€»æŸå¤±
total_loss = policy_loss + value_coef*value_loss + kl_penalty - entropy_bonus
```

#### é¢„çƒ­å¯åŠ¨å’ŒKLæ­£åˆ™åŒ–
```python
# è®¾ç½®å‚è€ƒç­–ç•¥
def set_reference_policy(reference_policy):
    self.reference_policy = reference_policy.eval()

# KLå‚è€ƒæŸå¤±
def compute_kl_reference_loss(states, hxs, masks):
    current_logits = self.policy.get_logits(states, hxs, masks)
    ref_logits = self.reference_policy.get_logits(states, hxs, masks)

    kl_div = KL(softmax(current_logits) || softmax(ref_logits))
    return kl_div.mean()

# åŠ¨æ€KLç³»æ•°è¡°å‡
kl_ref_coef = initial_coef * (1 - step/decay_steps) + final_coef * (step/decay_steps)
```

### RolloutBuffer (`src/core/algorithms/rollout_buffer.py`)

**åŠŸèƒ½**: å­˜å‚¨å’Œå¤„ç†PPOç»éªŒåºåˆ—

#### æ•°æ®å­˜å‚¨
```python
class RolloutBuffer:
    s: List[torch.Tensor]     # çŠ¶æ€åºåˆ—
    a: List[torch.Tensor]     # åŠ¨ä½œåºåˆ—
    logp: List[torch.Tensor]  # å¯¹æ•°æ¦‚ç‡åºåˆ—
    v: List[torch.Tensor]     # ä»·å€¼ä¼°è®¡åºåˆ—
    r: List[float]            # å¥–åŠ±åºåˆ—
    masks: List[float]        # ç»ˆæ­¢æ©ç åºåˆ—
```

#### GAEä¼˜åŠ¿ä¼°è®¡
```python
def compute_gae(last_value):
    # é€†å‘è®¡ç®—GAEä¼˜åŠ¿
    advantages = torch.zeros(rollout_len)
    last_gae = 0

    for t in reversed(range(rollout_len)):
        if t == rollout_len - 1:
            next_value = last_value
        else:
            next_value = values[t + 1]

        # TDè¯¯å·®
        delta = rewards[t] + gamma * next_value * masks[t] - values[t]

        # GAEé€’æ¨
        last_gae = delta + gamma * gae_lambda * masks[t] * last_gae
        advantages[t] = last_gae

    returns = advantages + values

    # ä¼˜åŠ¿æ ‡å‡†åŒ–
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    return states, actions, log_probs, values, returns, advantages
```

## âš™ï¸ é…ç½®ç³»ç»Ÿ

### PPOGlobalSchedulerModularConfig

**æ ¸å¿ƒé…ç½®ç»„**
```python
# ç½‘ç»œæ¶æ„
hidden_size: int = 128           # éšè—å±‚ç»´åº¦
layer_N: int = 2                # MLPå±‚æ•°
gru_layers: int = 2             # GRUå±‚æ•°
enable_decoupled_ac: bool = True # è§£è€¦Actor-Critic

# PPOè¶…å‚æ•°
lr: float = 5e-3                # å­¦ä¹ ç‡
gamma: float = 0.95             # æŠ˜æ‰£å› å­
gae_lambda: float = 0.95        # GAE lambda
clip_ratio: float = 0.15        # PPOè£å‰ªæ¯”ä¾‹
entropy_coef: float = 0.25      # ç†µç³»æ•°
value_coef: float = 0.5         # ä»·å€¼æƒé‡
epochs: int = 8                 # è®­ç»ƒè½®æ•°
rollout_len: int = 32           # å›åˆé•¿åº¦
minibatch_size: int = 64        # å°æ‰¹æ¬¡å¤§å°

# å¥–åŠ±ç³»ç»Ÿ
reward_mode: RewardMode = "hybrid"     # å¥–åŠ±æ¨¡å¼
throughput_target: float = 0.05        # ç›®æ ‡ååé‡
absolute_weight: float = 0.8           # ç»å¯¹åˆ†æ•°æƒé‡
delta_weight: float = 0.2              # å·®åˆ†åˆ†æ•°æƒé‡
alpha: float = 0.1                     # å»¶è¿Ÿæƒ©ç½šç³»æ•°
latency_threshold: float = 6.0         # å»¶è¿Ÿé˜ˆå€¼

# çŠ¶æ€æ„å»º
enable_enhanced_features: bool = True   # å¢å¼ºç‰¹å¾
max_queue_requests_per_replica: int = 4 # æ¯å‰¯æœ¬æœ€å¤§é˜Ÿåˆ—è¯·æ±‚æ•°
state_history_window: int = 5          # å†å²çª—å£
qps_window: int = 10                   # QPSè®¡ç®—çª—å£

# æ¸©åº¦æ§åˆ¶
enable_dynamic_temperature: bool = True # åŠ¨æ€æ¸©åº¦
base_temperature: float = 1.5          # åŸºç¡€æ¸©åº¦
min_temperature: float = 0.8           # æœ€å°æ¸©åº¦
max_temperature: float = 3.0           # æœ€å¤§æ¸©åº¦

# ç›‘æ§å’Œæ£€æŸ¥ç‚¹
enable_tensorboard: bool = True         # TensorBoardç›‘æ§
enable_checkpoints: bool = True         # æ£€æŸ¥ç‚¹ä¿å­˜
checkpoint_interval: int = 100         # ä¿å­˜é—´éš”
metrics_export_enabled: bool = False   # æŒ‡æ ‡å¯¼å‡º
```

### å¥–åŠ±æ¨¡å¼æšä¸¾
```python
class RewardMode(str, Enum):
    delta = "delta"      # å·®åˆ†å¥–åŠ±æ¨¡å¼
    instant = "instant"  # å³æ—¶å¥–åŠ±æ¨¡å¼
    hybrid = "hybrid"    # æ··åˆå¥–åŠ±æ¨¡å¼(æ¨è)
```

## ğŸ”§ å·¥å…·ç»„ä»¶

### çŠ¶æ€æ ‡å‡†åŒ– (`RunningNormalizer`)
```python
class RunningNormalizer:
    def update(self, x):
        # åœ¨çº¿æ›´æ–°å‡å€¼å’Œæ–¹å·®
        self.count += len(x)
        delta = x - self.mean
        self.mean += delta.sum() / self.count
        self.m2 += (delta * (x - self.mean)).sum()

    def normalize(self, x):
        var = self.m2 / max(self.count - 1, 1)
        std = sqrt(var + self.eps)
        return clip((x - self.mean) / std, -self.clip, self.clip)
```

### æ¸©åº¦æ§åˆ¶å™¨ (`TemperatureController`)
```python
class TemperatureController:
    def compute_temperature(self, current_qps, target_qps,
                          current_latency, target_latency,
                          system_load_balance, **kwargs):
        # QPSå‹åŠ›è®¡ç®—
        qps_pressure = (target_qps - current_qps) / target_qps

        # å»¶è¿Ÿå‹åŠ›è®¡ç®—
        latency_pressure = (current_latency - target_latency) / target_latency

        # ç»¼åˆå‹åŠ›
        total_pressure = (
            self.qps_sensitivity * qps_pressure +
            self.latency_sensitivity * latency_pressure
        )

        # æ¸©åº¦è°ƒæ•´
        temp = self.base_temperature * (1 + total_pressure)
        return clip(temp, self.min_temperature, self.max_temperature)
```

## ğŸ“Š ç›‘æ§ç³»ç»Ÿ

### TensorBoardé›†æˆ
```python
class TensorBoardMonitor:
    def log_training_metrics(self, stats, step):
        # PPOè®­ç»ƒæŒ‡æ ‡
        self.writer.add_scalar('PPO/PolicyLoss', stats['pi_loss'], step)
        self.writer.add_scalar('PPO/ValueLoss', stats['vf_loss'], step)
        self.writer.add_scalar('PPO/Entropy', stats['entropy'], step)

    def log_reward_metrics(self, reward_info, reward, step):
        # å¥–åŠ±ç»„ä»¶åˆ†è§£
        self.writer.add_scalar('Reward/Total', reward, step)
        self.writer.add_scalar('Reward/Throughput', reward_info['throughput'], step)
        self.writer.add_scalar('Reward/Latency', reward_info['latency'], step)
```

### æŒ‡æ ‡å¯¼å‡º
```python
class MetricsExporter:
    def append_training_metrics(self, step, metrics, metadata):
        # å¯¼å‡ºè®­ç»ƒæŒ‡æ ‡åˆ°CSV/Parquet
        record = {
            'step': step,
            'timestamp': time.time(),
            **metrics,
            **metadata
        }
        self.training_buffer.append(record)
```

## ğŸš€ è®­ç»ƒæµç¨‹

### ä¸»è®­ç»ƒå¾ªç¯ (`schedule()` æ–¹æ³•)

```python
def schedule() -> List[Tuple[int, Request]]:
    # 1. çŠ¶æ€æ„å»ºå’Œæ ‡å‡†åŒ–
    state = state_builder.build_global_state(replicas, schedulers, time, metrics)
    normalized_state = normalizer.normalize(state)

    # 2. å¥–åŠ±è®¡ç®—
    reward, reward_info = reward_calculator.calculate_reward(
        metrics, time, replica_ids, schedulers
    )

    # 3. åŠ¨ä½œé€‰æ‹© (å¸¦æ¸©åº¦æ§åˆ¶)
    temperature = temperature_controller.compute_temperature(...)
    action, log_prob, value, new_hxs = actor_critic.act_value(
        normalized_state, hidden_states, masks, temperature
    )

    # 4. ç»éªŒå­˜å‚¨
    rollout_buffer.add_step(
        state, action, log_prob, value, reward, masks
    )

    # 5. PPOæ›´æ–° (å½“ç¼“å†²åŒºæ»¡æ—¶)
    if rollout_buffer.is_full():
        # è®¡ç®—GAEä¼˜åŠ¿
        states, actions, log_probs, values, returns, advantages = \
            rollout_buffer.compute_gae(bootstrap_value)

        # PPOç­–ç•¥æ›´æ–°
        stats = ppo_trainer.update(
            states, actions, log_probs, values, returns, advantages,
            masks, hidden_states
        )

        # æ—¥å¿—è®°å½•
        tensorboard.log_training_metrics(stats, step)
        metrics_exporter.append_training_metrics(stats)

        # æ£€æŸ¥ç‚¹ä¿å­˜
        if should_save_checkpoint:
            checkpoint_manager.save_checkpoint(...)

    # 6. è¯·æ±‚åˆ†é…
    selected_replica = replica_ids[action]
    request = request_queue.pop(0)
    return [(selected_replica, request)]
```

### ç»Ÿè®¡ç¨³å®šåŒ–é˜¶æ®µ

åœ¨PPOè®­ç»ƒå¼€å§‹å‰æ‰§è¡Œç»Ÿè®¡æ”¶é›†é˜¶æ®µ:
```python
def statistics_stabilization_step():
    # 1. æ„å»ºçŠ¶æ€ (æ›´æ–°æ ‡å‡†åŒ–ç»Ÿè®¡)
    state = state_builder.build_global_state(...)
    normalizer.update(state)

    # 2. è®¡ç®—å¥–åŠ± (æ›´æ–°å¥–åŠ±ç»Ÿè®¡)
    reward, _ = reward_calculator.calculate_reward(...)

    # 3. éšæœºåŠ¨ä½œé€‰æ‹© (ä¸ä½¿ç”¨ç­–ç•¥ç½‘ç»œ)
    action = random.randint(0, num_replicas-1)

    # 4. è¯·æ±‚åˆ†é…
    return [(replica_ids[action], request)]
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§æŒ‡æ ‡

### è®­ç»ƒæŒ‡æ ‡
- **Policy Loss**: ç­–ç•¥ç½‘ç»œæŸå¤±
- **Value Loss**: ä»·å€¼ç½‘ç»œæŸå¤±
- **Entropy**: ç­–ç•¥ç†µ (æ¢ç´¢åº¦)
- **KL Divergence**: KLæ•£åº¦ (æ›´æ–°å¹…åº¦)
- **Clip Fraction**: è£å‰ªæ¯”ä¾‹
- **Explained Variance**: ä»·å€¼å‡½æ•°è§£é‡Šæ–¹å·®
- **Gradient Norm**: æ¢¯åº¦èŒƒæ•°

### ç³»ç»ŸæŒ‡æ ‡
- **Throughput**: ç³»ç»Ÿååé‡ (req/s)
- **Latency**: å¹³å‡å“åº”å»¶è¿Ÿ (s)
- **Queue Length**: å…¨å±€é˜Ÿåˆ—é•¿åº¦
- **Load Balance**: å‰¯æœ¬é—´è´Ÿè½½å‡è¡¡åº¦
- **Temperature**: å½“å‰æ¢ç´¢æ¸©åº¦
- **Reward Components**: å¥–åŠ±ç»„ä»¶åˆ†è§£

### çŠ¶æ€ç»Ÿè®¡
- **State Mean/Std**: çŠ¶æ€å‡å€¼å’Œæ ‡å‡†å·®
- **Utilization**: å‰¯æœ¬åˆ©ç”¨ç‡åˆ†å¸ƒ
- **Action Distribution**: åŠ¨ä½œé€‰æ‹©åˆ†å¸ƒ
- **QPS Metrics**: QPSè¶‹åŠ¿å’Œæ–¹å·®

## ğŸ”„ æ£€æŸ¥ç‚¹å’Œæ¨ç†

### æ£€æŸ¥ç‚¹ä¿å­˜
```python
checkpoint_data = {
    'step': current_step,
    'model_state_dict': actor_critic.state_dict(),
    'normalizer_state': normalizer.get_state(),
    'training_state': {'step': step, 'rollouts': rollouts},
    'metadata': {
        'reward_mode': mode,
        'state_builder_config': {...},
        'ppo_config': {...},
        'reward_config': {...}
    }
}
```

### æ¨ç†æ¨¡å¼
```python
# åŠ è½½æ£€æŸ¥ç‚¹è¿›å…¥æ¨ç†æ¨¡å¼
scheduler = PPOGlobalSchedulerModular(
    config_with_inference_only=True,
    load_checkpoint="path/to/checkpoint.pt"
)

# æ¨ç†æ—¶è¡Œä¸º
with torch.inference_mode():
    action, _, _, _ = actor_critic.act_value(state, hxs, masks, temperature)
    # è·³è¿‡æ‰€æœ‰è®­ç»ƒç›¸å…³æ“ä½œ
```

---

**â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€**
1. **æ¨¡å—åŒ–è®¾è®¡**: ç³»ç»Ÿé‡‡ç”¨æ¸…æ™°çš„æ¨¡å—åˆ†ç¦»ï¼Œæ¯ä¸ªç»„ä»¶éƒ½æœ‰æ˜ç¡®çš„èŒè´£å’Œæ¥å£ï¼Œä¾¿äºæµ‹è¯•å’Œç»´æŠ¤
2. **çŠ¶æ€å·¥ç¨‹**: é€šè¿‡StateBuilderå°†å¤æ‚çš„è°ƒåº¦å™¨çŠ¶æ€è½¬æ¢ä¸ºç»“æ„åŒ–ç‰¹å¾ï¼Œæ”¯æŒæ—¶åºå»ºæ¨¡å’Œè¶‹åŠ¿åˆ†æ
3. **å¥–åŠ±å¡‘é€ **: æ··åˆå¥–åŠ±æ¨¡å¼ç»“åˆç»å¯¹æ€§èƒ½å’Œæ”¹è¿›ä¿¡å·ï¼Œé¿å…äº†ä¼ ç»Ÿdeltaå¥–åŠ±çš„é™æ­¢çŠ¶æ€é—®é¢˜
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

è¯¥æ¶æ„é€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•å­¦ä¹ æœ€ä¼˜çš„è¯·æ±‚è°ƒåº¦ç­–ç•¥ï¼Œåœ¨ä¿è¯ç³»ç»Ÿæ€§èƒ½çš„åŒæ—¶å®ç°è´Ÿè½½å‡è¡¡å’Œèµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–ã€‚