#!/usr/bin/env python3
"""
è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”ç»“æœåˆ†æè„šæœ¬

ä»å„è°ƒåº¦å™¨çš„è¾“å‡ºæ—¥å¿—ä¸­æå–æ€§èƒ½æŒ‡æ ‡ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import re
import sys
import json
from pathlib import Path
from typing import Dict, Optional, Any

def extract_metrics_from_log(log_path: Path) -> Dict[str, Any]:
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–æ€§èƒ½æŒ‡æ ‡"""
    if not log_path.exists():
        return {"error": f"Log file not found: {log_path}"}

    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"Failed to read log: {e}"}

    metrics = {}

    # æå–å¹³å‡å»¶è¿Ÿ
    latency_patterns = [
        r'Average latency:\s*([\d.]+)',
        r'average_latency[:\s]+([\d.]+)',
        r'latency.*?(\d+\.\d+)',
        r'å»¶è¿Ÿ.*?(\d+\.\d+)',
    ]

    for pattern in latency_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metrics['average_latency'] = float(match.group(1))
            break

    # æå–ååé‡
    throughput_patterns = [
        r'Throughput:\s*([\d.]+)',
        r'throughput[:\s]+([\d.]+)',
        r'requests/second.*?(\d+\.\d+)',
        r'ååé‡.*?(\d+\.\d+)',
    ]

    for pattern in throughput_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metrics['throughput'] = float(match.group(1))
            break

    # æå–æ€»å¤„ç†æ—¶é—´
    duration_patterns = [
        r'Total simulation time:\s*([\d.]+)',
        r'execution time.*?(\d+\.\d+)',
        r'duration.*?(\d+\.\d+)',
    ]

    for pattern in duration_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            metrics['total_duration'] = float(match.group(1))
            break

    # å¦‚æœæ‰¾ä¸åˆ°æŒ‡æ ‡ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹æå–
    if not metrics:
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
        error_patterns = [
            r'Error.*',
            r'Exception.*',
            r'Traceback.*',
            r'Failed.*'
        ]

        for pattern in error_patterns:
            match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)
            if match:
                metrics['error'] = match.group(0)[:200]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
                break

    return metrics

def generate_comparison_report(results_dir: Path) -> str:
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""

    schedulers = {
        'PPO': 'ppo_output.log',
        'Random': 'random_output.log',
        'Round Robin': 'round_robin_output.log',
        'LOR': 'lor_output.log'
    }

    all_metrics = {}

    # æå–å„è°ƒåº¦å™¨æŒ‡æ ‡
    for scheduler_name, log_file in schedulers.items():
        log_path = results_dir / log_file
        metrics = extract_metrics_from_log(log_path)
        all_metrics[scheduler_name] = metrics

    # ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append("=" * 80)
    report.append("ğŸ“Š è°ƒåº¦å™¨æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
    report.append("=" * 80)
    report.append("")

    # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”è¡¨
    report.append("## æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
    report.append("")
    report.append(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡å»¶è¿Ÿ(s)':<12} {'ååé‡(req/s)':<15} {'çŠ¶æ€':<10}")
    report.append("-" * 60)

    valid_results = {}

    for scheduler_name in ['PPO', 'Random', 'Round Robin', 'LOR']:
        metrics = all_metrics[scheduler_name]

        if 'error' in metrics:
            status = "âŒ å¤±è´¥"
            latency_str = "N/A"
            throughput_str = "N/A"
        else:
            latency = metrics.get('average_latency', 'N/A')
            throughput = metrics.get('throughput', 'N/A')

            if latency != 'N/A' and throughput != 'N/A':
                status = "âœ… æˆåŠŸ"
                latency_str = f"{latency:.4f}"
                throughput_str = f"{throughput:.4f}"
                valid_results[scheduler_name] = metrics
            else:
                status = "âš ï¸ éƒ¨åˆ†"
                latency_str = f"{latency:.4f}" if latency != 'N/A' else "N/A"
                throughput_str = f"{throughput:.4f}" if throughput != 'N/A' else "N/A"

        report.append(f"{scheduler_name:<15} {latency_str:<12} {throughput_str:<15} {status:<10}")

    report.append("")

    # æ€§èƒ½æ’å
    if valid_results:
        report.append("## æ€§èƒ½æ’å")
        report.append("")

        # å»¶è¿Ÿæ’å (è¶Šä½è¶Šå¥½)
        if any('average_latency' in metrics for metrics in valid_results.values()):
            latency_ranking = sorted(
                [(name, metrics.get('average_latency', float('inf')))
                 for name, metrics in valid_results.items()
                 if 'average_latency' in metrics],
                key=lambda x: x[1]
            )

            report.append("### å¹³å‡å»¶è¿Ÿæ’å (è¶Šä½è¶Šå¥½):")
            for i, (name, latency) in enumerate(latency_ranking, 1):
                report.append(f"{i}. {name}: {latency:.4f}s")
            report.append("")

        # ååé‡æ’å (è¶Šé«˜è¶Šå¥½)
        if any('throughput' in metrics for metrics in valid_results.values()):
            throughput_ranking = sorted(
                [(name, metrics.get('throughput', 0))
                 for name, metrics in valid_results.items()
                 if 'throughput' in metrics],
                key=lambda x: x[1], reverse=True
            )

            report.append("### ååé‡æ’å (è¶Šé«˜è¶Šå¥½):")
            for i, (name, throughput) in enumerate(throughput_ranking, 1):
                report.append(f"{i}. {name}: {throughput:.4f} req/s")
            report.append("")

    # è¯¦ç»†ç»“æœ
    report.append("## è¯¦ç»†æµ‹è¯•ç»“æœ")
    report.append("")

    for scheduler_name, metrics in all_metrics.items():
        report.append(f"### {scheduler_name} è°ƒåº¦å™¨")
        if 'error' in metrics:
            report.append(f"âŒ æµ‹è¯•å¤±è´¥: {metrics['error']}")
        else:
            for key, value in metrics.items():
                if key != 'error':
                    report.append(f"- {key}: {value}")
        report.append("")

    # æµ‹è¯•ç¯å¢ƒä¿¡æ¯
    report.append("## æµ‹è¯•é…ç½®")
    report.append(f"- è¯·æ±‚æ•°é‡: {NUM_REQUESTS}")
    report.append(f"- QPS: {QPS}")
    report.append(f"- å‰¯æœ¬æ•°: {NUM_REPLICAS}")
    report.append(f"- æµ‹è¯•æ—¶é—´: {COMPARISON_ID}")
    report.append("")

    report.append("=" * 80)

    return "\n".join(report)

if __name__ == "__main__":
    results_dir = Path(sys.argv[1]) if len(sys.argv) > 1 else Path(".")

    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    import os
    global NUM_REQUESTS, QPS, NUM_REPLICAS, COMPARISON_ID
    NUM_REQUESTS = os.environ.get('NUM_REQUESTS', '500')
    QPS = os.environ.get('QPS', '2')
    NUM_REPLICAS = os.environ.get('NUM_REPLICAS', '4')
    COMPARISON_ID = os.environ.get('COMPARISON_ID', 'unknown')

    report = generate_comparison_report(results_dir)
    print(report)

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = results_dir.parent / f"scheduler_comparison_report_{COMPARISON_ID}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
