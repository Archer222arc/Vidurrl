#!/usr/bin/env python3
"""
ç»Ÿä¸€é¢„è®­ç»ƒç®¡ç†å™¨

æ•´åˆæ‰€æœ‰é¢„è®­ç»ƒåŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£å’Œé…ç½®ç®¡ç†ã€‚
æ”¯æŒï¼š
1. æ ‡å‡†BCè®­ç»ƒ (åŸpretrain_actor.pyåŠŸèƒ½)
2. å¢å¼ºç‹¬ç«‹é¢„è®­ç»ƒ (standalone_trainer.pyåŠŸèƒ½)
3. æ¨¡å‹éªŒè¯å’Œå…¼å®¹æ€§æ£€æŸ¥
4. çµæ´»çš„é…ç½®ç®¡ç†
"""

import argparse
import json
import pickle
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import torch
from torch.utils.tensorboard import SummaryWriter
import numpy as np

from .behavior_cloning_trainer import BehaviorCloningTrainer, DemoDataset, create_bc_trainer_from_config
from .model_validator import validate_pretrained_model


class UnifiedPretrainer:
    """ç»Ÿä¸€é¢„è®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.output_dir = Path(config.get('output_dir', './outputs/unified_pretrain'))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # è®­ç»ƒæ¨¡å¼
        self.training_mode = config.get('training_mode', 'standard')  # standard | enhanced

        # TensorBoard
        self.use_tensorboard = config.get('use_tensorboard', False)
        self.writer = None
        if self.use_tensorboard:
            self.writer = SummaryWriter(self.output_dir / "tensorboard")

        print(f"ğŸ”§ ç»Ÿä¸€é¢„è®­ç»ƒç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"   - è®­ç»ƒæ¨¡å¼: {self.training_mode}")
        print(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")

    def train_from_demo_files(
        self,
        demo_files: List[str],
        output_filename: str = "pretrained_model.pt"
    ) -> str:
        """ä»ç¤ºæ•™æ–‡ä»¶è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹"""

        if self.training_mode == 'standard':
            return self._train_standard_bc(demo_files, output_filename)
        elif self.training_mode == 'enhanced':
            return self._train_enhanced_bc(demo_files, output_filename)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {self.training_mode}")

    def _train_standard_bc(self, demo_files: List[str], output_filename: str) -> str:
        """æ ‡å‡†BCè®­ç»ƒ - å…¼å®¹åŸå§‹æ¥å£"""
        print(f"ğŸ“š [æ ‡å‡†æ¨¡å¼] å¼€å§‹BCè®­ç»ƒ...")

        # åŠ è½½æ‰€æœ‰ç¤ºæ•™æ•°æ®
        all_demo_data = []
        latest_stats = {}

        for demo_file in demo_files:
            with open(demo_file, 'rb') as f:
                data = pickle.load(f)
                all_demo_data.extend(data['demo_data'])
                if 'stats' in data:
                    latest_stats = data['stats']

        print(f"ğŸ“‚ åŠ è½½ç¤ºæ•™æ•°æ®å®Œæˆ: å…±{len(all_demo_data)}ä¸ªæ ·æœ¬")

        # æ›´æ–°é…ç½®
        if latest_stats:
            self.config.update(latest_stats)

        # åˆ›å»ºBCè®­ç»ƒå™¨
        trainer = create_bc_trainer_from_config(self.config)

        # åˆ›å»ºå¹¶è®­ç»ƒå½’ä¸€åŒ–å™¨ - ä¸PPOè®­ç»ƒä¿æŒä¸€è‡´
        from ....utils.normalizers import RunningNormalizer
        normalizer = RunningNormalizer(eps=1e-6, clip=5.0)

        # ç”¨æ‰€æœ‰çŠ¶æ€æ•°æ®è®­ç»ƒå½’ä¸€åŒ–å™¨
        for item in all_demo_data:
            state = np.array(item['state'], dtype=np.float32)
            normalizer.update(state)

        # è®¡ç®—æ–¹å·®å’Œæ ‡å‡†å·®ä»¥æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        var = normalizer.m2 / max(normalizer.count - 1, 1) if normalizer.m2 is not None else np.zeros_like(normalizer.mean)
        std = np.sqrt(np.maximum(var, normalizer.eps))
        print(f"ğŸ”§ å½’ä¸€åŒ–å™¨å·²è®­ç»ƒ: å‡å€¼èŒƒå›´={normalizer.mean.min():.3f}~{normalizer.mean.max():.3f}, æ ‡å‡†å·®èŒƒå›´={std.min():.3f}~{std.max():.3f}")

        # åˆ›å»ºæ•°æ®é›†
        dataset = DemoDataset(
            all_demo_data,
            normalizer=normalizer,
            augment_data=self.config.get('augment_data', False),
            noise_std=self.config.get('noise_std', 0.01)
        )

        # è®­ç»ƒ
        history = trainer.train(
            dataset=dataset,
            epochs=self.config.get('epochs', 30),
            batch_size=self.config.get('batch_size', 256),
            validation_split=self.config.get('validation_split', 0.1)
        )

        # ä¿å­˜æ¨¡å‹
        output_path = self.output_dir / output_filename
        trainer.save_pretrained_model(str(output_path))

        # ä¿å­˜è®­ç»ƒå†å²
        if self.use_tensorboard and self.writer:
            for epoch, (train_loss, train_acc, val_loss, val_acc) in enumerate(zip(
                history['train_loss'], history['train_acc'],
                history['val_loss'], history['val_acc']
            )):
                self.writer.add_scalars('Loss', {'Train': train_loss, 'Val': val_loss}, epoch)
                self.writer.add_scalars('Accuracy', {'Train': train_acc, 'Val': val_acc}, epoch)

        return str(output_path)

    def _train_enhanced_bc(self, demo_files: List[str], output_filename: str) -> str:
        """å¢å¼ºBCè®­ç»ƒ - ä½¿ç”¨standalone_trainerçš„åŠŸèƒ½"""
        print(f"ğŸš€ [å¢å¼ºæ¨¡å¼] å¼€å§‹ç‹¬ç«‹é¢„è®­ç»ƒ...")

        from .standalone_trainer import StandalonePretrainer

        # åˆ›å»ºå¢å¼ºé¢„è®­ç»ƒå™¨
        enhanced_config = self.config.copy()
        enhanced_config['output_dir'] = str(self.output_dir)

        pretrainer = StandalonePretrainer(enhanced_config)

        # æ‰§è¡Œè®­ç»ƒ
        history = pretrainer.train(demo_files)

        # å¤åˆ¶æœ€ä½³æ¨¡å‹åˆ°ç»Ÿä¸€è¾“å‡ºä½ç½®
        best_model_path = self.output_dir / "best_model.pt"
        output_path = self.output_dir / output_filename

        if best_model_path.exists():
            import shutil
            shutil.copy2(best_model_path, output_path)
            print(f"ğŸ“‚ æœ€ä½³æ¨¡å‹å·²å¤åˆ¶åˆ°: {output_path}")

        return str(output_path)

    def validate_model(self, model_path: str) -> bool:
        """éªŒè¯é¢„è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ” éªŒè¯é¢„è®­ç»ƒæ¨¡å‹: {model_path}")

        target_config = {
            'state_dim': self.config.get('state_dim'),
            'action_dim': self.config.get('action_dim'),
            'hidden_size': self.config.get('hidden_size'),
            'layer_N': self.config.get('layer_N'),
            'gru_layers': self.config.get('gru_layers'),
        }

        return validate_pretrained_model(model_path, target_config)

    def load_and_fine_tune(
        self,
        base_model_path: str,
        demo_files: List[str],
        output_filename: str = "fine_tuned_model.pt",
        fine_tune_epochs: int = 10
    ) -> str:
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒ"""
        print(f"ğŸ”§ å¼€å§‹å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹: {base_model_path}")

        # éªŒè¯åŸºç¡€æ¨¡å‹
        if not self.validate_model(base_model_path):
            raise ValueError("åŸºç¡€æ¨¡å‹éªŒè¯å¤±è´¥")

        # åŠ è½½æ‰€æœ‰ç¤ºæ•™æ•°æ®
        all_demo_data = []
        for demo_file in demo_files:
            with open(demo_file, 'rb') as f:
                data = pickle.load(f)
                all_demo_data.extend(data['demo_data'])

        # åˆ›å»ºBCè®­ç»ƒå™¨å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        trainer = create_bc_trainer_from_config(self.config)
        trainer.load_pretrained_model(base_model_path)

        # åˆ›å»ºæ•°æ®é›†ï¼ˆå¯ç”¨æ•°æ®å¢å¼ºï¼‰
        dataset = DemoDataset(
            all_demo_data,
            augment_data=True,
            noise_std=self.config.get('noise_std', 0.01)
        )

        # å¾®è°ƒè®­ç»ƒ
        print(f"ğŸ¯ å¼€å§‹å¾®è°ƒè®­ç»ƒ: {fine_tune_epochs} epochs")
        history = trainer.train(
            dataset=dataset,
            epochs=fine_tune_epochs,
            batch_size=self.config.get('batch_size', 256),
            validation_split=self.config.get('validation_split', 0.1)
        )

        # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
        output_path = self.output_dir / output_filename
        trainer.save_pretrained_model(str(output_path))

        return str(output_path)

    def create_standard_config(
        self,
        state_dim: int,
        action_dim: int,
        hidden_size: int = 128,
        epochs: int = 30
    ) -> Dict[str, Any]:
        """åˆ›å»ºæ ‡å‡†é…ç½®"""
        return {
            'training_mode': 'standard',
            'state_dim': state_dim,
            'action_dim': action_dim,
            'hidden_size': hidden_size,
            'layer_N': 2,
            'gru_layers': 2,
            'epochs': epochs,
            'batch_size': 256,
            'learning_rate': 1e-3,
            'validation_split': 0.1,
            'device': 'cpu',
            'augment_data': False,
        }

    def create_enhanced_config(
        self,
        state_dim: int,
        action_dim: int,
        hidden_size: int = 256,
        epochs: int = 100
    ) -> Dict[str, Any]:
        """åˆ›å»ºå¢å¼ºé…ç½®"""
        return {
            'training_mode': 'enhanced',
            'state_dim': state_dim,
            'action_dim': action_dim,
            'hidden_size': hidden_size,
            'layer_N': 3,
            'gru_layers': 2,
            'learning_rate': 1e-4,
            'weight_decay': 1e-5,
            'batch_size': 512,
            'epochs': epochs,
            'validation_split': 0.2,
            'early_stopping_patience': 20,
            'save_interval': 10,
            'device': 'cpu',
            'augment_data': True,
            'noise_std': 0.01,
            'use_tensorboard': True,
        }

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if self.writer:
            self.writer.close()


def main():
    """ç»Ÿä¸€é¢„è®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€é¢„è®­ç»ƒç®¡ç†å™¨")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--demo-files", nargs='+', required=True, help="ç¤ºæ•™æ•°æ®æ–‡ä»¶åˆ—è¡¨")
    parser.add_argument("--output", type=str, default="pretrained_model.pt", help="è¾“å‡ºæ¨¡å‹æ–‡ä»¶å")
    parser.add_argument("--mode", choices=['standard', 'enhanced'], default='standard', help="è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--base-model", type=str, help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¾®è°ƒï¼‰")
    parser.add_argument("--fine-tune-epochs", type=int, default=10, help="å¾®è°ƒè½®æ•°")

    # å¿«é€Ÿé…ç½®é€‰é¡¹
    parser.add_argument("--state-dim", type=int, help="çŠ¶æ€ç»´åº¦")
    parser.add_argument("--action-dim", type=int, help="åŠ¨ä½œç»´åº¦")
    parser.add_argument("--hidden-size", type=int, help="éšè—å±‚å¤§å°")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--device", type=str, default="cpu", help="è®­ç»ƒè®¾å¤‡")

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    if args.config and Path(args.config).exists():
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = {}

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.mode:
        config['training_mode'] = args.mode
    if args.state_dim:
        config['state_dim'] = args.state_dim
    if args.action_dim:
        config['action_dim'] = args.action_dim
    if args.hidden_size:
        config['hidden_size'] = args.hidden_size
    if args.epochs:
        config['epochs'] = args.epochs
    if args.device:
        config['device'] = args.device

    # éªŒè¯å¿…è¦å‚æ•°
    required_params = ['state_dim', 'action_dim']
    missing_params = [p for p in required_params if p not in config]
    if missing_params:
        print(f"âŒ ç¼ºå°‘å¿…è¦å‚æ•°: {missing_params}")
        print("è¯·é€šè¿‡é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°æä¾›")
        return

    # åˆ›å»ºç»Ÿä¸€é¢„è®­ç»ƒå™¨
    pretrainer = UnifiedPretrainer(config)

    try:
        if args.base_model:
            # å¾®è°ƒæ¨¡å¼
            output_path = pretrainer.load_and_fine_tune(
                base_model_path=args.base_model,
                demo_files=args.demo_files,
                output_filename=args.output,
                fine_tune_epochs=args.fine_tune_epochs
            )
            print(f"ğŸ‰ å¾®è°ƒå®Œæˆ! è¾“å‡º: {output_path}")
        else:
            # æ ‡å‡†è®­ç»ƒæ¨¡å¼
            output_path = pretrainer.train_from_demo_files(
                demo_files=args.demo_files,
                output_filename=args.output
            )
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! è¾“å‡º: {output_path}")

        # éªŒè¯è¾“å‡ºæ¨¡å‹
        if pretrainer.validate_model(output_path):
            print("âœ… è¾“å‡ºæ¨¡å‹éªŒè¯é€šè¿‡")
        else:
            print("âŒ è¾“å‡ºæ¨¡å‹éªŒè¯å¤±è´¥")

    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()