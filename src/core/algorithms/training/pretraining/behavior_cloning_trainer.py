#!/usr/bin/env python3
"""
è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨ - ç»Ÿä¸€çš„é¢„è®­ç»ƒæ ¸å¿ƒæ¨¡å—

æ•´åˆåŸå§‹pretrain_actor.pyçš„åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„BCè®­ç»ƒæ¥å£ã€‚
"""

import pickle
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from ....models.actor_critic import ActorCritic
from ....utils.normalizers import RunningNormalizer


class DemoDataset(Dataset):
    """ç¤ºæ•™æ•°æ®é›† - å…¼å®¹åŸå§‹æ ¼å¼"""

    def __init__(
        self,
        demo_data: List[Dict[str, Any]],
        normalizer: Optional[RunningNormalizer] = None,
        augment_data: bool = False,
        noise_std: float = 0.01
    ):
        self.demo_data = demo_data
        self.normalizer = normalizer
        self.augment_data = augment_data
        self.noise_std = noise_std

        # é¢„å¤„ç†æ•°æ®
        self.states = []
        self.actions = []
        self.weights = []
        self._preprocess_data()

    def _preprocess_data(self):
        """é¢„å¤„ç†æ•°æ®"""
        strategy_counts = {}

        for item in self.demo_data:
            state = np.array(item['state'], dtype=np.float32)
            action = item['action']
            strategy = item.get('strategy', 'unknown')

            # å½’ä¸€åŒ–çŠ¶æ€
            if self.normalizer:
                state = self.normalizer.normalize(state)

            self.states.append(state)
            self.actions.append(action)
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        # è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆå¹³è¡¡ä¸åŒç­–ç•¥ï¼‰
        total_samples = len(self.demo_data)
        num_strategies = len(strategy_counts)

        for item in self.demo_data:
            strategy = item.get('strategy', 'unknown')
            if num_strategies > 1:
                strategy_weight = total_samples / (num_strategies * strategy_counts[strategy])
            else:
                strategy_weight = 1.0
            self.weights.append(strategy_weight)

        print(f"ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆ: {total_samples} æ ·æœ¬, ç­–ç•¥åˆ†å¸ƒ: {strategy_counts}")

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        state = self.states[idx].copy()
        action = self.actions[idx]
        weight = self.weights[idx]

        # æ•°æ®å¢å¼º
        if self.augment_data and self.noise_std > 0:
            noise = np.random.normal(0, self.noise_std, state.shape)
            state = state + noise

        return torch.tensor(state, dtype=torch.float32), action, weight


class BehaviorCloningTrainer:
    """è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨ - ç»Ÿä¸€æ¥å£"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_size: int = 128,
        layer_N: int = 2,
        gru_layers: int = 2,
        enable_decoupled: bool = False,
        feature_projection_dim: Optional[int] = None,
        learning_rate: float = 1e-3,
        device: str = "cpu"
    ):
        self.device = torch.device(device)
        self.action_dim = action_dim

        # æ¨¡å‹é…ç½®
        self.model_config = {
            'state_dim': state_dim,
            'action_dim': action_dim,
            'hidden_size': hidden_size,
            'layer_N': layer_N,
            'gru_layers': gru_layers,
            'enable_decoupled': enable_decoupled,
            'feature_projection_dim': feature_projection_dim,
        }

        # åˆ›å»ºActor-Criticç½‘ç»œ
        self.actor_critic = ActorCritic(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_size=hidden_size,
            layer_N=layer_N,
            gru_layers=gru_layers,
            enable_decoupled=enable_decoupled,
            feature_projection_dim=feature_projection_dim,
        ).to(self.device)

        # è®¾ç½®ä¼˜åŒ–å™¨ - åªè®­ç»ƒActorå‚æ•°
        actor_params = []
        if self.actor_critic.enable_decoupled:
            # è§£è€¦æ¶æ„ - ç›´æ¥è®¿é—®ï¼Œç¼ºå¤±å±æ€§ä¼šè‡ªç„¶æŠ¥é”™
            actor_params.extend(self.actor_critic.actor_branch.parameters())
            actor_params.extend(self.actor_critic.actor_head.parameters())
        else:
            # è€¦åˆæ¶æ„ - ç›´æ¥è®¿é—®ï¼Œç¼ºå¤±å±æ€§ä¼šè‡ªç„¶æŠ¥é”™
            actor_params.extend(self.actor_critic.actor.parameters())

        self.optimizer = torch.optim.Adam(actor_params, lr=learning_rate)

        print(f"ğŸ¤– BCè®­ç»ƒå™¨å·²åˆ›å»º: {self.model_config}")
        print(f"   - Actorå‚æ•°æ•°é‡: {sum(p.numel() for p in actor_params)}")

    def train_epoch(self, dataloader: DataLoader) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.actor_critic.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for states, actions, weights in dataloader:
            states = states.to(self.device)

            # å®‰å…¨çš„tensorè½¬æ¢å¹¶æ¸…ç†æ— æ•ˆaction
            if isinstance(actions, torch.Tensor):
                actions = actions.long().to(self.device)
            else:
                actions = torch.tensor(actions, dtype=torch.long).to(self.device)

            # æ¸…ç†æ— æ•ˆçš„action indices
            invalid_action_mask = (actions < 0) | (actions >= self.actor_critic.action_dim)
            if invalid_action_mask.any():
                invalid_count = invalid_action_mask.sum().item()
                print(f"WARNING: Found {invalid_count} invalid actions in training data, fixing...")
                actions = torch.clamp(actions, 0, self.actor_critic.action_dim - 1)

            if isinstance(weights, torch.Tensor):
                weights = weights.float().to(self.device)
            else:
                weights = torch.tensor(weights, dtype=torch.float32).to(self.device)

            # åˆå§‹åŒ–éšè—çŠ¶æ€ - æ­£ç¡®çš„GRUæ ¼å¼: (num_layers, batch_size, hidden_size)
            batch_size = states.shape[0]
            hxs = torch.zeros(self.actor_critic.gru_layers, batch_size, self.actor_critic.hidden_size,
                            device=self.device)
            mask = torch.ones(batch_size, 1, device=self.device)

            # å‰å‘ä¼ æ’­ - è·å–raw logits (ä¿ç•™æ¢¯åº¦)
            action_logits, _ = self.actor_critic.forward_actor_logits(states, hxs, mask)

            # è®¡ç®—åŠ æƒäº¤å‰ç†µæŸå¤±
            log_probs = F.log_softmax(action_logits, dim=-1)
            loss = F.nll_loss(log_probs, actions.long(), reduction='none')
            weighted_loss = (loss * weights).mean()

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            weighted_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=1.0)
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += weighted_loss.item()
            pred = action_logits.argmax(dim=1)
            correct += (pred == actions).sum().item()
            total += actions.size(0)

        return total_loss / len(dataloader), correct / total if total > 0 else 0.0

    def validate(self, dataloader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹"""
        self.actor_critic.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for states, actions, weights in dataloader:
                states = states.to(self.device)

                # å®‰å…¨çš„tensorè½¬æ¢
                if isinstance(actions, torch.Tensor):
                    actions = actions.long().to(self.device)
                else:
                    actions = torch.tensor(actions, dtype=torch.long).to(self.device)

                if isinstance(weights, torch.Tensor):
                    weights = weights.float().to(self.device)
                else:
                    weights = torch.tensor(weights, dtype=torch.float32).to(self.device)

                batch_size = states.shape[0]
                hxs = torch.zeros(self.actor_critic.gru_layers, batch_size, self.actor_critic.hidden_size,
                                device=self.device)
                mask = torch.ones(batch_size, 1, device=self.device)

                action_logits, _ = self.actor_critic.forward_actor_logits(states, hxs, mask)

                # ç¡®ä¿ action_logits æ˜¯æµ®ç‚¹ç±»å‹
                action_logits = action_logits.float()

                log_probs = F.log_softmax(action_logits, dim=1)
                loss = F.nll_loss(log_probs, actions.long(), reduction='none')
                weighted_loss = (loss * weights).mean()

                total_loss += weighted_loss.item()
                pred = action_logits.argmax(dim=1)
                correct += (pred == actions).sum().item()
                total += actions.size(0)

        return total_loss / len(dataloader), correct / total if total > 0 else 0.0

    def train(
        self,
        dataset: DemoDataset,
        epochs: int = 10,
        batch_size: int = 256,
        validation_split: float = 0.1
    ) -> Dict[str, List[float]]:
        """æ‰§è¡Œè®­ç»ƒ"""
        # æ•°æ®é›†åˆ†å‰²
        dataset_size = len(dataset)
        val_size = int(dataset_size * validation_split) if validation_split > 0 else 0
        train_size = dataset_size - val_size

        if val_size > 0:
            train_dataset, val_dataset = torch.utils.data.random_split(
                dataset, [train_size, val_size]
            )
        else:
            train_dataset = dataset
            val_dataset = None

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) if val_dataset else None

        # è®­ç»ƒå†å²
        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

        print(f"ğŸš€ å¼€å§‹BCè®­ç»ƒ: {epochs} epochs, æ‰¹å¤§å°: {batch_size}")
        print(f"   - è®­ç»ƒé›†: {train_size} æ ·æœ¬")
        if val_size > 0:
            print(f"   - éªŒè¯é›†: {val_size} æ ·æœ¬")

        for epoch in range(epochs):
            start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)

            # éªŒè¯
            if val_loader:
                val_loss, val_acc = self.validate(val_loader)
                history['val_loss'].append(val_loss)
                history['val_acc'].append(val_acc)
            else:
                val_loss, val_acc = train_loss, train_acc
                history['val_loss'].append(val_loss)
                history['val_acc'].append(val_acc)

            # è¾“å‡ºè¿›åº¦
            epoch_time = time.time() - start_time
            print(f"Epoch {epoch+1:3d}/{epochs} | "
                  f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f} | "
                  f"Time: {epoch_time:.1f}s")

        print("âœ… BCè®­ç»ƒå®Œæˆ")
        return history

    def save_pretrained_model(self, output_path: str) -> None:
        """ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹ - å…¼å®¹åŸå§‹æ ¼å¼"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨ç»Ÿä¸€çš„ä¿å­˜æ ¼å¼
        save_dict = {
            'model_state_dict': self.actor_critic.state_dict(),
            'model_config': self.model_config,
            'training_metadata': {
                'save_time': datetime.now().isoformat(),
                'model_type': 'behavior_cloning',
                'architecture': 'actor_critic',
            }
        }

        torch.save(save_dict, output_path)
        print(f"ğŸ’¾ é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {output_path}")

    def load_pretrained_model(self, model_path: str) -> None:
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)

        # å…¼å®¹ä¸åŒçš„ä¿å­˜æ ¼å¼
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint

        # åŠ è½½æƒé‡
        missing_keys, unexpected_keys = self.actor_critic.load_state_dict(state_dict, strict=False)

        if missing_keys:
            print(f"âš ï¸ ç¼ºå°‘çš„é”®: {missing_keys}")
        if unexpected_keys:
            print(f"âš ï¸ æ„å¤–çš„é”®: {unexpected_keys}")

        print(f"ğŸ“‚ é¢„è®­ç»ƒæ¨¡å‹å·²åŠ è½½: {model_path}")


def create_bc_trainer_from_config(config: Dict[str, Any]) -> BehaviorCloningTrainer:
    """ä»é…ç½®åˆ›å»ºBCè®­ç»ƒå™¨"""
    return BehaviorCloningTrainer(
        state_dim=config['state_dim'],
        action_dim=config['action_dim'],
        hidden_size=config.get('hidden_size', 128),
        layer_N=config.get('layer_N', 2),
        gru_layers=config.get('gru_layers', 2),
        enable_decoupled=config.get('enable_decoupled', False),
        feature_projection_dim=config.get('feature_projection_dim', None),
        learning_rate=config.get('learning_rate', 1e-3),
        device=config.get('device', 'cpu')
    )


def train_bc_from_demo_file(
    demo_file: str,
    output_path: str,
    config: Dict[str, Any]
) -> BehaviorCloningTrainer:
    """ä»ç¤ºæ•™æ–‡ä»¶è®­ç»ƒBC - ä¾¿æ·å‡½æ•°"""
    # åŠ è½½ç¤ºæ•™æ•°æ®
    with open(demo_file, 'rb') as f:
        data = pickle.load(f)

    demo_data = data['demo_data']
    stats = data.get('stats', {})

    print(f"ğŸ“‚ åŠ è½½ç¤ºæ•™æ•°æ®: {demo_file}")
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {stats}")

    # æ›´æ–°é…ç½®
    if 'state_dim' in stats:
        config['state_dim'] = stats['state_dim']
    if 'action_dim' in stats:
        config['action_dim'] = stats['action_dim']

    # åˆ›å»ºæ•°æ®é›†å’Œè®­ç»ƒå™¨
    dataset = DemoDataset(demo_data, augment_data=config.get('augment_data', False))
    trainer = create_bc_trainer_from_config(config)

    # è®­ç»ƒ
    history = trainer.train(
        dataset=dataset,
        epochs=config.get('epochs', 10),
        batch_size=config.get('batch_size', 256),
        validation_split=config.get('validation_split', 0.1)
    )

    # ä¿å­˜æ¨¡å‹
    trainer.save_pretrained_model(output_path)

    return trainer