#!/usr/bin/env python3
"""
ç²¾ç¡®è°ƒè¯•PPOæ¢¯åº¦é—®é¢˜çš„è„šæœ¬
"""

import torch
import numpy as np
from src.rl_components import ActorCritic, PPOTrainer, RolloutBuffer

def debug_ppo_components():
    """é€æ­¥è°ƒè¯•PPOç»„ä»¶çš„æ¢¯åº¦è®¡ç®—"""

    print("ğŸ” PPOæ¢¯åº¦è°ƒè¯• - é€æ­¥éªŒè¯")
    print("=" * 60)

    device = "cpu"
    state_dim = 81
    action_dim = 2
    hidden_size = 128
    rollout_len = 8

    # 1. åˆ›å»ºActor-Critic
    ac = ActorCritic(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_size=hidden_size,
        layer_N=2,
        gru_layers=2,
        use_orthogonal=True,
    ).to(device)

    # ç¡®ä¿è®­ç»ƒæ¨¡å¼
    ac.train()

    print(f"1. Actor-Criticè®­ç»ƒæ¨¡å¼: {ac.training}")

    # æ£€æŸ¥å‚æ•°æ˜¯å¦éœ€è¦æ¢¯åº¦
    param_count = 0
    for name, param in ac.named_parameters():
        if param.requires_grad:
            param_count += 1
    print(f"2. éœ€è¦æ¢¯åº¦çš„å‚æ•°æ•°é‡: {param_count}")

    # 2. åˆ›å»ºPPO trainer
    ppo = PPOTrainer(
        ac,
        lr=0.001,
        clip_ratio=0.2,
        entropy_coef=0.01,
        value_coef=0.5,
        epochs=4,
        minibatch_size=8,  # å°batchç”¨äºæµ‹è¯•
        max_grad_norm=0.5,
        device=device,
    )

    print(f"3. PPO traineråˆ›å»ºæˆåŠŸ")

    # 3. åˆ›å»ºrollout bufferå¹¶æ·»åŠ ç»éªŒ
    buffer = RolloutBuffer(
        state_dim=state_dim,
        rollout_len=rollout_len,
        gamma=0.95,
        gae_lambda=0.95,
        device=device,
    )

    # åˆå§‹åŒ–hidden states
    hxs = torch.zeros(2, 1, hidden_size, device=device)

    print(f"4. æ¨¡æ‹Ÿrolloutè¿‡ç¨‹...")

    # æ¨¡æ‹Ÿrollout
    for step in range(rollout_len):
        # åˆ›å»ºéšæœºçŠ¶æ€
        state = torch.randn(state_dim, device=device)
        mask = torch.ones(1, device=device)

        # å‰å‘ä¼ æ’­è·å–åŠ¨ä½œå’Œä»·å€¼
        with torch.no_grad():
            action, logp, value, hxs = ac.act_value(state.unsqueeze(0), hxs, mask.unsqueeze(-1))

        # æ·»åŠ åˆ°buffer
        buffer.add_step(
            s=state,
            a=action.squeeze(0),
            logp=logp.squeeze(0),
            v=value.squeeze(0),
            r=0.1 * step,  # ç®€å•å¥–åŠ±
            mask=mask.squeeze(0)
        )

    print(f"5. Rolloutå®Œæˆï¼ŒbufferåŒ…å« {buffer.ptr} æ­¥ç»éªŒ")

    # 4. è®¡ç®—GAE
    with torch.no_grad():
        # æœ€ç»ˆçŠ¶æ€çš„ä»·å€¼ä¼°è®¡
        final_state = torch.randn(1, state_dim, device=device)
        final_mask = torch.ones(1, 1, device=device)
        z = ac.forward_mlp(final_state)
        z, _ = ac.forward_gru(z, hxs, final_mask)
        last_v = ac.critic(z).squeeze(-1)

    s_t, a_t, logp_t, v_t, ret_t, adv_t = buffer.compute_gae(last_v)
    masks = torch.tensor(buffer.masks, dtype=torch.float32, device=device)

    print(f"6. GAEè®¡ç®—å®Œæˆ")
    print(f"   çŠ¶æ€tensor shape: {s_t.shape}, requires_grad: {s_t.requires_grad}")
    print(f"   åŠ¨ä½œtensor shape: {a_t.shape}, requires_grad: {a_t.requires_grad}")
    print(f"   ä¼˜åŠ¿tensor shape: {adv_t.shape}, requires_grad: {adv_t.requires_grad}")

    # 5. æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡PPOæ›´æ–°æ­¥éª¤
    print(f"7. æ‰‹åŠ¨æ‰§è¡ŒPPOæ›´æ–°...")

    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    ac.train()

    # è·å–batchæ•°æ®
    bs = s_t[:8]  # å–å‰8ä¸ªæ ·æœ¬
    ba = a_t[:8].view(-1).to(torch.long)
    blogp = logp_t[:8]
    bret = ret_t[:8]
    badv = adv_t[:8]
    bm = masks[:8]

    print(f"   batchçŠ¶æ€: shape={bs.shape}, requires_grad={bs.requires_grad}")

    # åˆ›å»ºæ–°çš„hidden states
    with torch.no_grad():
        batch_hxs = torch.zeros(2, 8, hidden_size, device=device)

    print(f"   batch hidden states: requires_grad={batch_hxs.requires_grad}")

    # å‰å‘ä¼ æ’­
    print(f"8. æ‰§è¡Œå‰å‘ä¼ æ’­...")
    new_logp, entropy, v_pred, _ = ac.evaluate_actions(bs, batch_hxs, bm.unsqueeze(-1), ba)

    print(f"   new_logp: requires_grad={new_logp.requires_grad}")
    print(f"   entropy: requires_grad={entropy.requires_grad}")
    print(f"   v_pred: requires_grad={v_pred.requires_grad}")

    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¾“å‡ºæœ‰æ¢¯åº¦
    if not any([new_logp.requires_grad, entropy.requires_grad, v_pred.requires_grad]):
        print("âŒ æ‰€æœ‰è¾“å‡ºéƒ½æ²¡æœ‰æ¢¯åº¦ï¼")

        # å°è¯•ç›´æ¥æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        print("9. ç›´æ¥æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        test_state = torch.randn(1, state_dim, device=device, requires_grad=True)
        test_hxs = torch.zeros(2, 1, hidden_size, device=device)
        test_mask = torch.ones(1, 1, device=device)
        test_action = torch.randint(0, action_dim, (1,), device=device)

        print(f"   test_state requires_grad: {test_state.requires_grad}")

        test_logp, test_entropy, test_v, _ = ac.evaluate_actions(test_state, test_hxs, test_mask, test_action)

        print(f"   ç›´æ¥æµ‹è¯•ç»“æœ:")
        print(f"     logp requires_grad: {test_logp.requires_grad}")
        print(f"     entropy requires_grad: {test_entropy.requires_grad}")
        print(f"     value requires_grad: {test_v.requires_grad}")

        if test_logp.requires_grad:
            print("âœ… ç›´æ¥æµ‹è¯•æˆåŠŸï¼é—®é¢˜åœ¨äºè¾“å…¥tensor")
        else:
            print("âŒ ç›´æ¥æµ‹è¯•ä¹Ÿå¤±è´¥ï¼é—®é¢˜åœ¨äºæ¨¡å‹æœ¬èº«")

            # æ›´æ·±å±‚è°ƒè¯•
            print("10. æ·±å±‚æ¨¡å‹è°ƒè¯•...")
            ac.eval()
            print(f"    è®¾ç½®evalæ¨¡å¼åï¼Œtraining: {ac.training}")

            ac.train()
            print(f"    è®¾ç½®trainæ¨¡å¼åï¼Œtraining: {ac.training}")

            # æ£€æŸ¥ç‰¹å®šå±‚
            for name, module in ac.named_modules():
                if hasattr(module, 'training'):
                    print(f"    {name}: training={module.training}")
                    if name in ['gru', 'actor', 'critic']:
                        break
    else:
        print("âœ… æ¢¯åº¦è®¡ç®—æ­£å¸¸ï¼")

        # ç»§ç»­æµ‹è¯•lossè®¡ç®—
        print(f"9. æµ‹è¯•lossè®¡ç®—...")

        # PPO loss
        ratio = torch.exp(new_logp - blogp)
        surr1 = ratio * badv
        surr2 = torch.clamp(ratio, 1.0 - 0.2, 1.0 + 0.2) * badv
        pi_loss = -torch.min(surr1, surr2).mean()

        # Value loss
        v_clipped = v_t[:8] + (v_pred - v_t[:8]).clamp(-0.2, 0.2)
        vf_loss1 = (v_pred - bret).pow(2)
        vf_loss2 = (v_clipped - bret).pow(2)
        vf_loss = 0.5 * torch.max(vf_loss1, vf_loss2).mean()

        # Total loss
        loss = pi_loss + 0.5 * vf_loss - 0.01 * entropy

        print(f"   loss requires_grad: {loss.requires_grad}")

        if loss.requires_grad:
            print("10. æµ‹è¯•åå‘ä¼ æ’­...")
            loss.backward()
            print("âœ… åå‘ä¼ æ’­æˆåŠŸï¼")
        else:
            print("âŒ lossæ²¡æœ‰æ¢¯åº¦")


if __name__ == "__main__":
    debug_ppo_components()